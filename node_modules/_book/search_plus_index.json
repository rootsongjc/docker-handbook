{"./":{"url":"./","title":"序言","keywords":"","body":"Istio Handbook——Istio 服务网格进阶实战 注意：本书已转移到 servicemesher/istio-handbook，将由 ServiceMesher 社区共同撰写，本仓库已不再维护。 Istio 是由 Google、IBM、Lyft 等共同开源的 Service Mesh（服务网格）框架，于2017年初开始进入大众视野。本书是 Kubernetes Handbook——Kubernetes 中文指南/云原生应用架构实践手册的续篇，Kubernetes 解决了云原生应用的部署问题，Istio 解决是应用的服务（流量）治理问题。在 Kubernetes Handbook 中有大量的关于 Istio 和服务网格相关的章节，这些内容已经与 Kubernetes 本身没有太强的关系，随着 2018年7月31日 Istio 1.0 发布，Istio 本身已经日趋稳定，作为 ServiceMesher 社区的联合创始人我也在社区活动中积累的大量的资料，为了回馈社区，我决定组合社区中已有的资料加上个人撰写，将服务网格部分独立出来单独成书。 本书的主题包括： 服务网格概念解析 控制平面和数据平面的原理 Istio 架构详解 Istio 进阶实战 基于 Istio 的自定义扩展 本书基于 Istio 1.0+ 版本编写，您可以通过以下地址参与到本书的编写或阅读本书： GitHub 地址：https://github.com/rootsongjc/istio-handbook Gitbook 在线浏览：https://jimmysong.io/istio-handbook/ 快速开始 阅读本书前希望您有容器和 Kubernetes 的基础知识，如果您想要从零开始，那么可以使用 kubernetes-vagrant-centos-cluster 并运行 Bookinfo 应用来快速体验服务网格。 致谢 ServiceMesher 负责翻译了 Envoy 官方文档，并负责了 Istio 官方中文文档的维护，同时还编译了大量资料，本书所有文章的文末参考栏目里标注了参考文章的链接，感谢大家对本书的大力支持。 版权 本书概念图，封面图片上海静安寺夜景，Jimmy Song 摄。 本书发行版权归属于电子工业出版社博文视点，未经授权请勿私自印刷发行。 参与本书 请参考 Istio 网站样式指南。 ServiceMesher 更多关于服务网格的资讯、技术干货请关注我们的社区。社区网站：https://www.servicemesher.com Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:35:26 "},"preface/service-mesh-the-microservices-in-post-kubernetes-era.html":{"url":"preface/service-mesh-the-microservices-in-post-kubernetes-era.html","title":"服务网格——后 Kubernetes 时代的微服务","keywords":"","body":"服务网格——后 Kubernetes 时代的微服务 我想听说过 Service Mesh 并试用过 Istio 的人可能都会有以下几个疑问： 为什么 Istio 一定要绑定 Kubernetes 呢？ Kubernetes 和 Service Mesh 分别在云原生中扮演什么角色？ Istio 扩展了 Kubernetes 的哪些方面？解决了哪些问题？ Kubernetes、Envoy（xDS 协议）与 Istio 之间又是什么关系？ 到底该不该上 Service Mesh？ 本文试图带您梳理清楚 Kubernetes、Envoy（xDS 协议）以及 Istio Service Mesh 之间的关系及内在联系。本文介绍了 Kubernetes 中的负载均衡方式，Envoy 的 xDS 协议对于 Service Mesh 的意义以及为什么说有了 Kubernetes 还需要 Istio。 使用 Service Mesh 并不是说与 Kubernetes 决裂，而是水到渠成的事情。Kubernetes 的本质是通过声明式配置对应用进行生命周期管理，而 Service Mesh 的本质是应用间的流量和安全性管理。假如你已经使用 Kubernetes 构建了稳定的微服务平台，那么如何设置服务间调用的负载均衡和流量控制？ Envoy 对于 Service Mesh 或者说 Cloud Native 最大的贡献就是定义了 xDS，Envoy 虽然本质上是一个 proxy，但是它的配置协议被众多开源软件所支持，如 Istio、Linkerd、AWS App Mesh、SOFAMesh 等。 关于标题 2018年9月1日，Bilgin Ibryam 在 InfoQ 发表了一篇文章 Microservices in a Post-Kubernetes Era，中文版见后 Kubernetes 时代的微服务（译文有些错误，仅供参考）。本文标题中虽然没有明确指明”后 Kubernetes 时代的微服务“是什么，但是从文中可以看出作者的观点是：在后 Kubernetes 时代，服务网格（Service Mesh）技术已完全取代了使用软件库实现网络运维（例如 Hystrix 断路器）的方式。本文索性就借用该标题。 本节包含以下内容 说明 kube-proxy 的作用。 Kubernetes 在微服务管理上的局限性。 介绍下 Istio Service Mesh 的功能。 介绍下 xDS 包含哪些内容。 比较了 Kubernetes、Envoy 和 Istio Service Mesh 中的一些概念。 重要观点 如果你没有心里阅读下文的所有内容，那么可以先阅读看下下面列出的本文中的一些主要观点： Kubernetes 的本质是应用的生命周期管理，具体说是部署和管理（扩缩容、自动恢复、发布）。 Kubernetes 为微服务提供了可扩展、高弹性的部署和管理平台。 Service Mesh 的基础是透明代理，通过 sidecar proxy 拦截到微服务间流量后再通过控制平面配置管理微服务的行为。 Service Mesh 将流量管理从 Kubernetes 中解耦，Service Mesh 内部的流量无需 kube-proxy 组件的支持，通过为更接近微服务应用层的抽象，管理服务间的流量、安全性和可观察性。 Envoy xDS 定义了 Service Mesh 配置的协议标准。 Service Mesh 是对 Kubernetes 中的 service 更上层的抽象，它的下一步是 serverless。 阅读本文之前 推荐大家在阅读本文之前希望您对微服务、容器和 Kubernetes 有一定认识，如果您已经阅读过以下几篇文章将对您理解本文更有帮助，本文中也引用过了下面文章中的部分观点。 深入解读 Service Mesh 背后的技术细节 by 刘超 Istio流量管理实现机制深度解析 by 赵化冰 Service Mesh架构反思：数据平面和控制平面的界线该如何划定？by 敖小剑 理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 by 宋净超 Service Mesh 深度学习系列——Istio源码分析之pilot-agent模块分析 by 丁轶群 Kubernetes vs Service Mesh 下图展示的是 Kubernetes 与 Service Mesh 中的的服务访问关系，本文仅针对 sidecar per-pod 模式，详情请参考服务网格的实现模式。 图片 - kubernetes vs service mesh Kubernetes 集群的每个节点都部署了一个 kube-proxy 组件，该组件会与 Kubernetes API Server 通信，获取集群中的 service 信息，然后设置 iptables 规则，直接将对某个 service 的请求发送到对应的 Endpoint（属于同一组 service 的 pod）上。 Istio Service Mesh 中沿用了 Kubernetes 中的 service 做服务注册，通过 Control Plane 来生成数据平面的配置（使用 CRD 声明，保存在 etcd 中），数据平面的透明代理（transparent proxy）以 sidecar 容器的形式部署在每个应用服务的 pod 中，这些 proxy 都需要请求 Control Plane 来同步代理配置，之所以说是透明代理，是因为应用程序容器完全无感知代理的存在，该过程 kube-proxy 组件一样需要拦截流量，只不过 kube-proxy 拦截的是进出 Kubernetes 节点的流量，而 sidecar proxy 拦截的是进出该 Pod 的流量，详见理解 Istio Service Mesh 中 Envoy Sidecar 代理的路由转发。 Service Mesh 的劣势 因为 Kubernetes 每个节点上都会运行众多的 Pod，将原先 kube-proxy 方式的路由转发功能置于每个 pod 中，这将导致大量的配置分发、同步和最终一致性问题。为了细粒度的机型流量管理，必将代理一系列新的抽象，增加了用户的心智负担，但随着技术的普及慢慢将得到缓解。 Service Mesh 的优势 kube-proxy 的设置都是全局生效的，无法对每个服务做细粒度的控制，而 Service Mesh 通过 sidecar proxy 的方式将 Kubernetes 中对流量的控制从 service 一层抽离出来，可以做更多的扩展。 kube-proxy 组件 在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式。 在 Kubernetes v1.0 版本，代理完全在 userspace 实现。Kubernetes v1.1 版本新增了 iptables 代理模式，但并不是默认的运行模式。从 Kubernetes v1.2 起，默认使用 iptables 代理。在 Kubernetes v1.8.0-beta.0 中，添加了 ipvs 代理模式。关于 kube-proxy 组件的更多介绍请参考 kubernetes 简介：service 和 kube-proxy 原理 和 使用 IPVS 实现 Kubernetes 入口流量负载均衡。 kube-proxy 的缺陷 在上面的链接中作者指出了 kube-proxy 的不足之处： 首先，如果转发的 pod 不能正常提供服务，它不会自动尝试另一个 pod，当然这个可以通过 liveness probes 来解决。每个 pod 都有一个健康检查的机制，当有 pod 健康状况有问题时，kube-proxy 会删除对应的转发规则。另外，nodePort 类型的服务也无法添加 TLS 或者更复杂的报文路由机制。 Kube-proxy 实现了流量在 Kubernetes service 多个 pod 实例间的负载均衡，但是如何对这些 service 间的流量做细粒度的控制，比如按照百分比划分流量到不同的应用版本（这些应用都属于同一个 service，但位于不同的 deployment 上），做金丝雀发布（灰度发布）和蓝绿发布？Kubernetes 社区给出了 使用 Deployment 做金丝雀发布的方法，该方法本质上就是通过修改 pod 的 label 来将不同的 pod 划归到 Deployment 的 Service 上。 Kubernetes Ingress vs Istio Gateway Kubernetes 中的 Ingress 资源对象跟 Istio Service Mesh 中的 Gateway 的功能类似，都是负责集群南北流量（从集群外部进入集群内部的流量）。 kube-proxy 只能路由 Kubernetes 集群内部的流量，而我们知道 Kubernetes 集群的 Pod 位于 CNI 创建的外网络中，集群外部是无法直接与其通信的，因此 Kubernetes 中创建了 ingress 这个资源对象，它由位于 Kubernetes 边缘节点（这样的节点可以是很多个也可以是一组）的 Ingress controller 驱动，负责管理南北向流量（从集群外部进入 Kubernetes 集群的流量），Ingress 必须对接各种个 Ingress Controller 才能使用，比如 nginx ingress controller、traefik。Ingress 只适用于 HTTP 流量，使用方式也很简单，只能对 service、port、HTTP 路径等有限字段匹配来路由流量，这导致它无法路由如 MySQL、redis 和各种私有 RPC 等 TCP 流量。要想直接路由南北向的流量，只能使用 Service 的 LoadBalancer 或 NodePort，前者需要云厂商支持而且可能需要付费，后者需要进行额外的端口管理。有些 Ingress controller 支持暴露 TCP 和 UDP 服务，但是只能使用 Service 来暴露，Ingress 本身是不支持的，例如 nginx ingress controller，服务的暴露的端口是通过创建 ConfigMap 的方式来配置的。 Istio Gateway 描述的负载均衡器用于承载进出网格边缘的连接。该规范中描述了一系列开放端口和这些端口所使用的协议、负载均衡的 SNI 配置等内容。Gateway 是一种 CRD 扩展，它同时复用了 Envoy proxy 的能力，详细配置请参考 Istio 官网。 xDS 协议 下面这张图大家在了解 Service Mesh 的时候可能都看到过，每个方块代表一个服务的示例，例如 Kubernetes 中的一个 Pod（其中包含了 sidecar proxy），xDS 协议控制了 Istio Service Mesh 中所有流量的具体行为，即将下图中的方块链接到了一起。 图片 - Service Mesh 示意图 xDS 协议是由 Envoy 提出的，在 Envoy v2 版本 API 中最原始的 xDS 协议只指 CDS、EDS、LDS 和 RDS。 下面我们以两个 service，每个 service 都有两个实例的例子来看下 Envoy 的 xDS 协议。 图片 - Envoy xDS 协议 上图中的箭头不是流量在进入 Enovy Proxy 后的路径或路由，而是想象的一种 Envoy 中 xDS 接口处理的顺序并非实际顺序，其实 xDS 之间也是有交叉引用的。 Envoy 通过查询文件或管理服务器来动态发现资源。概括地讲，对应的发现服务及其相应的 API 被称作 xDS。Envoy 通过订阅（subscription）方式来获取资源，订阅方式有以下三种： 文件订阅：监控指定路径下的文件，发现动态资源的最简单方式就是将其保存于文件，并将路径配置在 ConfigSource 中的 path 参数中。 gRPC 流式订阅：每个 xDS API 可以单独配置 ApiConfigSource，指向对应的上游管理服务器的集群地址。 轮询 REST-JSON 轮询订阅：单个 xDS API 可对 REST 端点进行的同步（长）轮询。 以上的 xDS 订阅方式详情请参考 xDS 协议解析。Istio 使用的 gRPC 流式订阅的方式配置所有的数据平面的 sidecar proxy。 关于 xDS 协议的详细分解请参考丁轶群博士的这几篇文章： Service Mesh深度学习系列part1—istio源码分析之pilot-agent模块分析 Service Mesh深度学习系列part2—istio源码分析之pilot-discovery模块分析 Service Mesh深度学习系列part3—istio源码分析之pilot-discovery模块分析（续） 文章中介绍了 Istio pilot 的总体架构、Envoy 配置的生成、pilot-discovery 模块的功能，以及 xDS 协议中的 CDS、EDS 及 ADS，关于 ADS 详情请参考 Enovy 官方文档。 xDS 协议要点 最后总结下关于 xDS 协议的要点： CDS、EDS、LDS、RDS 是最基础的 xDS 协议，它们可以分别独立更新的。 所有的发现服务（Discovery Service）可以连接不同的 Management Server，也就是说管理 xDS 的服务器可以是多个。 Envoy 在原始 xDS 协议的基础上进行了一些列扩充，增加了 SDS（秘钥发现服务）、ADS（聚合发现服务）、HDS（健康发现服务）、MS（Metric 服务）、RLS（速率限制服务）等 API。 为了保证数据一致性，若直接使用 xDS 原始 API 的话，需要保证这样的顺序更新：CDS --> EDS --> LDS --> RDS，这是遵循电子工程中的先合后断（Make-Before-Break）原则，即在断开原来的连接之前先建立好新的连接，应用在路由里就是为了防止设置了新的路由规则的时候却无法发现上游集群而导致流量被丢弃的情况，类似于电路里的断路。 CDS 设置 Service Mesh 中有哪些服务。 EDS 设置哪些实例（Endpoint）属于这些服务（Cluster）。 LDS 设置实例上监听的端口以配置路由。 RDS 最终服务间的路由关系，应该保证最后更新 RDS。 Envoy Envoy 是 Istio Service Mesh 中默认的 Sidecar，Istio 在 Enovy 的基础上按照 Envoy 的 xDS 协议扩展了其控制平面，在讲到 Envoy xDS 协议之前还需要我们先熟悉下 Envoy 的基本术语。下面列举了 Envoy 里的基本术语及其数据结构解析，关于 Envoy 的详细介绍请参考 Envoy 官方文档，至于 Envoy 在 Service Mesh（不仅限于 Istio） 中是如何作为转发代理工作的请参考网易云刘超的这篇深入解读 Service Mesh 背后的技术细节 以及理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持，本文引用其中的一些观点，详细内容不再赘述。 图片 - Envoy proxy 架构图 基本术语 下面是您应该了解的 Enovy 里的基本术语： Downstream（下游）：下游主机连接到 Envoy，发送请求并接收响应，即发送请求的主机。 Upstream（上游）：上游主机接收来自 Envoy 的连接和请求，并返回响应，即接受请求的主机。 Listener（监听器）：监听器是命名网地址（例如，端口、unix domain socket 等)，下游客户端可以连接这些监听器。Envoy 暴露一个或者多个监听器给下游主机连接。 Cluster（集群）：集群是指 Envoy 连接的一组逻辑相同的上游主机。Envoy 通过服务发现来发现集群的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到集群的哪个成员。 Envoy 中可以设置多个 Listener，每个 Listener 中又可以设置 filter chain（过滤器链表），而且过滤器是可扩展的，这样就可以更方便我们操作流量的行为，例如设置加密、私有 RPC 等。 xDS 协议是由 Envoy 提出的，现在是 Istio 中默认的 sidecar proxy，但只要实现 xDS 协议理论上都是可以作为 Istio 中的 sidecar proxy 的，例如蚂蚁金服开源的 SOFAMosn 和 nginx 开源的 nginmesh。 Istio Service Mesh 图片 - Istio service mesh 架构图 Istio 是一个功能十分丰富的 Service Mesh，它包括如下功能： 流量管理：这是 Istio 的最基本的功能。 策略控制：通过 Mixer 组件和各种适配器来实现，实现访问控制系统、遥测捕获、配额管理和计费等。 可观测性：通过 Mixer 来实现。 安全认证：Citadel 组件做密钥和证书管理。 Istio 中的流量管理 Istio 中定义了如下的 CRD 来帮助用户进行流量管理： Gateway：Gateway 描述了在网络边缘运行的负载均衡器，用于接收传入或传出的HTTP / TCP连接。 VirtualService：VirtualService 实际上将 Kubernetes 服务连接到 Istio Gateway。它还可以执行更多操作，例如定义一组流量路由规则，以便在主机被寻址时应用。 DestinationRule：DestinationRule 所定义的策略，决定了经过路由处理之后的流量的访问策略。简单的说就是定义流量如何路由。这些策略中可以定义负载均衡配置、连接池尺寸以及外部检测（用于在负载均衡池中对不健康主机进行识别和驱逐）配置。 EnvoyFilter：EnvoyFilter 对象描述了针对代理服务的过滤器，这些过滤器可以定制由 Istio Pilot 生成的代理配置。这个配置初级用户一般很少用到。 ServiceEntry：默认情况下 Istio Service Mesh 中的服务是无法发现 Mesh 外的服务的，ServiceEntry 能够在 Istio 内部的服务注册表中加入额外的条目，从而让网格中自动发现的服务能够访问和路由到这些手工加入的服务。 Kubernetes vs Envoy xDS vs Istio 在阅读完上文对 Kubernetes 的 kube-proxy 组件、Envoy xDS 和 Istio 中流量管理的抽象概念之后，下面将带您仅就流量管理方面比较下三者对应的组件/协议（注意，三者不可以完全等同）。 Kubernetes Envoy xDS Istio Service Mesh Endpoint Endpoint - Service Route VirtualService kube-proxy Route DestinationRule kube-proxy Listener EnvoyFilter Ingress Listener Gateway Service Cluster ServiceEntry 总结 如果说 Kubernetes 管理的对象是 Pod，那么 Service Mesh 中管理的对象就是一个个 Service，所以说使用 Kubernetes 管理微服务后再应用 Service Mesh 就是水到渠成了，如果连 Service 你也不像管了，那就用如 knative 这样的 serverless 平台，这就是后话了。 Envoy 的功能也不只是做流量转发，以上概念只不过是 Istio 在 Kubernetes 之上新增一层抽象层中的冰山一角，但因为流量管理是服务网格最基础也是最重要的功能，所以这将成为本书的开始。 参考 Istio 流量管理的基本概念详解 - jimmysong.io Kubernetes kube-proxy 中的 iptables 代理模式 - jimmysong.io Kubernetes kube-proxy 中的 ipvs 代理模式 - jimmysong.io Envoy v2 API 概览 - servicemesher.com 监听器发现服务（LDS）- servicemesher.com 路由发现服务（RDS）- servicemesher.com 集群发现服务（CDS）- servicemesher.com Kubernetes service - jimmysong.io xDS 协议解析 - jimmysong.io 深入解读 Service Mesh 背后的技术细节 - cnblogs.com 理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 - jimmysong.io kubernetes 简介：service 和 kube-proxy 原理 - cizixs.com 使用 IPVS 实现 Kubernetes 入口流量负载均衡 - jishu.io Istio 流量管理实现机制深度解析 - zhaohuabing.com 企业级服务网格架构之路解读 - jimmysong.io 调试 Envoy 和 Pilot - istio.io Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"concepts/what-is-service-mesh.html":{"url":"concepts/what-is-service-mesh.html","title":"什么是服务网格？","keywords":"","body":"什么是服务网格？ Service mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。 服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。—— Willian Morgan Buoyant CEO 服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。 服务网格的特点 服务网格有如下几个特点： 应用程序间通讯的中间层 轻量级网络代理 应用程序无感知 解耦应用程序的重试/超时、监控、追踪和服务发现 目前两款流行的服务网格开源软件 Linkerd 和 Istio 都可以直接在 kubernetes 中集成，其中 Linkerd 已经成为 CNCF 成员，Istio 在 2018年7月31日宣布 1.0。 理解服务网格 如果用一句话来解释什么是服务网格，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用服务网格也就无须关系服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给服务网格就可以了。 Phil Calçado 在他的这篇博客 Pattern: Service Mesh 中详细解释了服务网格的来龙去脉： 从最原始的主机之间直接使用网线相连 网络层的出现 集成到应用程序内部的控制流 分解到应用程序外部的控制流 应用程序的中集成服务发现和断路器 出现了专门用于服务发现和断路器的软件包/库，如 Twitter 的 Finagle 和 Facebook 的 Proxygen，这时候还是集成在应用程序内部 出现了专门用于服务发现和断路器的开源软件，如 Netflix OSS、Airbnb 的 synapse 和 nerve 最后作为微服务的中间层服务网格出现 服务网格的架构如下图所示： 图片 - Service Mesh 架构图 图片来自：Pattern: Service Mesh 服务网格作为 sidecar 运行，对应用程序来说是透明，所有应用程序间的流量都会通过它，所以对应用程序流量的控制都可以在 serivce mesh 中实现。 服务网格如何工作？ 下面以 Istio 为例讲解服务网格如何在 Kubernetes 中工作。 Istio 将服务请求路由到目的地址，根据中的参数判断是到生产环境、测试环境还是 staging 环境中的服务（服务可能同时部署在这三个环境中），是路由到本地环境还是公有云环境？所有的这些路由信息可以动态配置，可以是全局配置也可以为某些服务单独配置。 当 Istio 确认了目的地址后，将流量发送到相应服务发现端点，在 Kubernetes 中是 service，然后 service 会将服务转发给后端的实例。 Istio 根据它观测到最近请求的延迟时间，选择出所有应用程序的实例中响应最快的实例。 Istio 将请求发送给该实例，同时记录响应类型和延迟数据。 如果该实例挂了、不响应了或者进程不工作了，Istio 将把请求发送到其他实例上重试。 如果该实例持续返回 error，Istio 会将该实例从负载均衡池中移除，稍后再周期性得重试。 如果请求的截止时间已过，Istio 主动失败该请求，而不是再次尝试添加负载。 Istio 以 metric 和分布式追踪的形式捕获上述行为的各个方面，这些追踪信息将发送到集中 metric 系统。 为何使用服务网格？ 服务网格并没有给我们带来新功能，它是用于解决其他工具已经解决过的问题，只不过这次是在云原生的 Kubernetes 环境下的实现。 在传统的 MVC 三层 Web 应用程序架构下，服务之间的通讯并不复杂，在应用程序内部自己管理即可，但是在现今的复杂的大型网站情况下，单体应用被分解为众多的微服务，服务之间的依赖和通讯十分复杂，出现了 twitter 开发的 Finagle、Netflix 开发的 Hystrix 和 Google 的 Stubby 这样的 “胖客户端” 库，这些就是早期的服务网格，但是它们都近适用于特定的环境和特定的开发语言，并不能作为平台级的服务网格支持。 在云原生架构下，容器的使用给予了异构应用程序的更多可行性，Kubernetes 增强的应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，同时开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。 服务网格全景图 Service Mesh 的概念于 2016 年诞生至今仍在蓬勃发展，下面是由于 ServiceMesher 社区维护的Service Mesh 列表，如您发现该列表中有所遗漏欢迎到 servicemesher/awesome-servicemesh 上提交 PR。 amalgam8 - 用于异构微服务的基于版本的路由网格 ambassador - 开源的基于 Envoy proxy 构建的用于微服务的 Kubernetes 原生 API 网关 https://www.getambassador.io aspen-mesh - 隶属于 F5 的公司开发的 Service Mesh conduit - 适用于 Kubernetes 的轻量级 Service Mesh https://conduit.io consul - Consul 一种分布式、高可用的和数据中心感知解决方案，用于跨动态分布式基础架构连接和配置应用程序。https://www.consul.io/ dubbo - Apache Dubbo™ (incubating)是一款高性能Java RPC框架。http://dubbo.incubator.apache.org envoy - C++ 前端/服务代理 https://www.envoyproxy.io istio - 用于连接、保护、控制和观测服务。 kong - 云原生 API 网关 https://konghq.com/install linkerd - 云原生应用的开源 Service Mesh https://linkerd.io mesher - 华为开源的基于轻量级基于 go chassis 的 Service Mesh。 nginmesh - 基于 Nginx 的 Service Mesh nginx-unit - NGINX Unit is a new, lightweight, open source application server built to meet the demands of today’s dynamic and distributed applications. servicecomb - ServiceComb 是华为开源的微服务框架，提供便捷的在云中开发和部署应用的方式。 sofa-mesh - SOFAMesh 是蚂蚁金服开源的基于 Istio 的大规模服务网格解决方案。 http://www.sofastack.tech/ sofa-mosn - SOFAMosn 是由蚂蚁金服开源的一个模块化可观测的智能网络，可用作为 sidecar 部署在 Service Mesh 中。http://www.sofastack.tech tars - Tars 是腾讯开源的基于名称服务的高性能 RPC 框架。使用 tars 协议并提供半自动化运维平台。 参考 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? - buoyant.io Istio: A service mesh for AWS ECS - medium.com 初次了解 Istio - istio.io Application Network Functions With ESBs, API Management, and Now.. Service Mesh? - blog.christianposta.com Pattern: Service Mesh - philcalcado.com Envoy 官方文档中文版 - servicemesher.com Istio 官方文档 - istio.io servicemesher/awesome-servicemesh - github.com Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"concepts/service-mesh-architectures.html":{"url":"concepts/service-mesh-architectures.html","title":"服务网格架构","keywords":"","body":"服务网格架构 下图是Conduit Service Mesh（现在已合并到Linkerd2中了）的架构图，这是Service Mesh的一种典型的架构。 图片 - 服务网格架构示意图 服务网格中分为控制平面和数据平面，当前流行的两款开源的服务网格 Istio 和 Linkerd 实际上都是这种构造，只不过 Istio 的划分更清晰，而且部署更零散，很多组件都被拆分，控制平面中包括 Mixer、Pilot、Citadel，数据平面默认是用Envoy；而 Linkerd 中只分为 Linkerd 做数据平面，namerd 作为控制平面。 控制平面 控制平面的特点： 不直接解析数据包 与控制平面中的代理通信，下发策略和配置 负责网络行为的可视化 通常提供API或者命令行工具可用于配置版本化管理，便于持续集成和部署 数据平面 数据平面的特点： 通常是按照无状态目标设计的，但实际上为了提高流量转发性能，需要缓存一些数据，因此无状态也是有争议的 直接处理入站和出站数据包，转发、路由、健康检查、负载均衡、认证、鉴权、产生监控数据等 对应用来说透明，即可以做到无感知部署 参考 企业级服务网格架构之路解读 Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"concepts/service-mesh-patterns.html":{"url":"concepts/service-mesh-patterns.html","title":"服务网格的实现模式","keywords":"","body":"服务网格的实现模式 我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成Service Mesh架构前使用微服务架构通常的形式，下图是使用Service Mesh架构的最终形式。 图片 - Service Mesh 架构示意图 当然在达到这一最终形态之前我们需要将架构一步步演进，下面给出的是参考的演进路线。 Ingress或边缘代理 如果你使用的是Kubernetes做容器编排调度，那么在进化到Service Mesh架构之前，通常会使用Ingress Controller，做集群内外流量的反向代理，如使用Traefik或Nginx Ingress Controller。 图片 - Ingress 或边缘代理架构示意图 这样只要利用Kubernetes的原有能力，当你的应用微服务化并容器化需要开放外部访问且只需要L7代理的话这种改造十分简单，但问题是无法管理服务间流量。 路由器网格 Ingress或者边缘代理可以处理进出集群的流量，为了应对集群内的服务间流量管理，我们可以在集群内加一个Router层，即路由器层，让集群内所有服务间的流量都通过该路由器。 图片 - 路由器网格架构示意图 这个架构无需对原有的单体应用和新的微服务应用做什么改造，可以很轻易的迁移进来，但是当服务多了管理起来就很麻烦。 Proxy per Node 这种架构是在每个节点上都部署一个代理，如果使用Kubernetes来部署的话就是使用DaemonSet对象，Linkerd第一代就是使用这种方式部署的，一代的Linkerd使用Scala开发，基于JVM比较消耗资源，二代的Linkerd使用Go开发。 图片 - Proxy per node 架构示意图 这种架构有个好处是每个节点只需要部署一个代理即可，比起在每个应用中都注入一个sidecar的方式更节省资源，而且更适合基于物理机/虚拟机的大型单体应用，但是也有一些副作用，比如粒度还是不够细，如果一个节点出问题，该节点上的所有服务就都会无法访问，对于服务来说不是完全透明的。 Sidecar代理/Fabric模型 这个一般不会成为典型部署类型，当企业的服务网格架构演进到这一步时通常只会持续很短时间，然后就会增加控制平面。跟前几个阶段最大的不同就是，应用程序和代理被放在了同一个部署单元里，可以对应用程序的流量做更细粒度的控制。 图片 - Sidecar代理/Fabric模型示意图 这已经是最接近Service Mesh架构的一种形态了，唯一缺的就是控制平面了。所有的sidecar都支持热加载，配置的变更可以很容易的在流量控制中反应出来，但是如何操作这么多sidecar就需要一个统一的控制平面了。 Sidecar代理/控制平面 下面的示意图是目前大多数Service Mesh的架构图，也可以说是整个Service Mesh架构演进的最终形态。 图片 - Sidecar 代理/控制平面架构示意图 这种架构将代理作为整个服务网格中的一部分，使用Kubernetes部署的话，可以通过以sidecar的形式注入，减轻了部署的负担，可以对每个服务的做细粒度权限与流量控制。但有一点不好就是为每个服务都注入一个代理会占用很多资源，因此要想方设法降低每个代理的资源消耗。 多集群部署和扩展 以上都是单个服务网格集群的架构，所有的服务都位于同一个集群中，服务网格管理进出集群和集群内部的流量，当我们需要管理多个集群或者是引入外部的服务时就需要网格扩展和多集群配置。 参考 企业级服务网格架构之路解读 Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"concepts/istio-architecture.html":{"url":"concepts/istio-architecture.html","title":"Istio 架构解析","keywords":"","body":"Istio 架构解析 下面是以漫画的形式说明 Istio 是什么。 图片 - 来自 Twitter @daniseyu21 该图中描绘了以下内容： Istio 可以在虚拟机和容器中运行 Istio 的组成 Pilot：服务发现、流量管理 Mixer：访问控制、遥测 Citadel：终端用户认证、流量加密 Service mesh 关注的方面 可观察性 安全性 可运维性 Istio 是可定制可扩展的，组建是可拔插的 Istio 作为控制平面，在每个服务中注入一个 Envoy 代理以 Sidecar 形式运行来拦截所有进出服务的流量，同时对流量加以控制 应用程序应该关注于业务逻辑（这才能生钱），非功能性需求交给 Service Mesh 参考 Isito 是什么? - istio.io Bookinfo 示例 Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"concepts/sidecar-pattern.html":{"url":"concepts/sidecar-pattern.html","title":"Sidecar 模式","keywords":"","body":"Sidecar 模式 Sidecar 模式是 Istio 服务网格采用的模式，在服务网格出现之前该模式就一直存在，尤其是当微服务出现后开始盛行，本文讲解 Sidecar 模式。 什么是 Sidecar 模式 将应用程序的功能划分为单独的进程可以被视为 Sidecar 模式。Sidecar 设计模式允许你为应用程序添加许多功能，而无需额外第三方组件的配置和代码。 就如 Sidecar 连接着摩托车一样，类似地在软件架构中， Sidecar 应用是连接到父应用并且为其扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。 让我用一个例子解释一下。想象一下假如你有6个微服务相互通信以确定一个包裹的成本。 每个微服务都需要具有可观察性、监控、日志记录、配置、断路器等功能。所有这些功能都是根据一些行业标准的第三方库在每个微服务中实现的。 但再想一想，这不是多余吗？它不会增加应用程序的整体复杂性吗？如果你的应用程序是用不同的语言编写时会发生什么——如何合并那些特定用于 .Net、Java、Python 等语言的第三方库。 使用 Sidecar 模式的优势 通过抽象出与功能相关的共同基础设施到一个不同层降低了微服务代码的复杂度。 因为你不再需要编写相同的第三方组件配置文件和代码，所以能够降低微服务架构中的代码重复度。 降低应用程序代码和底层平台的耦合度。 Sidecar 模式如何工作 Sidecar 是容器应用模式的一种，也是在 Service Mesh 中发扬光大的一种模式，详见 Service Mesh 架构解析，其中详细描述了节点代理和 Sidecar 模式的 Service Mesh 架构。 使用 Sidecar 模式部署服务网格时，无需在节点上运行代理（因此您不需要基础结构的协作），但是集群中将运行多个相同的 Sidecar 副本。从另一个角度看：我可以为一组微服务部署到一个服务网格中，你也可以部署一个有特定实现的服务网格。在 Sidecar 部署方式中，你会为每个应用的容器部署一个伴生容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边运行一个 Sidecar 容器，可以理解为两个容器共享存储、网络等资源，可以广义的将这个注入了 Sidecar 容器的 Pod 理解为一台主机，两个容器共享主机资源。 例如下图 SOFAMesh & SOFA MOSN—基于Istio构建的用于应对大规模流量的Service Mesh解决方案的架构图中描述的，MOSN 作为 Sidecar 的方式和应用运行在同一个 Pod 中，拦截所有进出应用容器的流量，SOFAMesh 兼容 Istio，其中使用 Go 语言开发的 SOFAMosn 替换了 Envoy。 图片 - SOFAMesh 架构图 参考 理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 - jimmysong.io 微服务中的 Sidecar 设计模式解析 - servicemesher.com Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"concepts/sidecar-injection-deep-dive.html":{"url":"concepts/sidecar-injection-deep-dive.html","title":"Istio 中的 Sidecar 注入与流量劫持详解","keywords":"","body":"Istio 中的 Sidecar 注入与流量劫持详解 在讲解 Istio 如何将 Envoy 代理注入到应用程序 Pod 中之前，我们需要先了解以下几个概念： Sidecar 模式：容器应用模式之一，Service Mesh 架构的一种实现方式。 Init 容器：Pod 中的一种专用的容器，在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。 iptables：流量劫持是通过 iptables 转发实现的。 Init 容器 Init 容器是一种专用容器，它在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。 一个 Pod 中可以指定多个 Init 容器，如果指定了多个，那么 Init 容器将会按顺序依次运行。只有当前面的 Init 容器必须运行成功后，才可以运行下一个 Init 容器。当所有的 Init 容器运行完成后，Kubernetes 才初始化 Pod 和运行应用容器。 Init 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。 在 Pod 启动过程中，Init 容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出。如果由于运行时或失败退出，将导致容器启动失败，它会根据 Pod 的 restartPolicy 指定的策略进行重试。然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用 RestartPolicy 策略。 在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。Init 容器的端口将不会在 Service 中进行聚集。 正在初始化中的 Pod 处于 Pending 状态，但应该会将 Initializing 状态设置为 true。Init 容器运行完成以后就会自动终止。 关于 Init 容器的详细信息请参考 Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册。 Sidecar 注入示例分析 我们看下 Istio 官方示例 bookinfo 中 productpage 的 YAML 配置，关于 bookinfo 应用的详细 YAML 配置请参考 bookinfo.yaml。 apiVersion: v1 kind: Service metadata: name: productpage labels: app: productpage spec: ports: - port: 9080 name: http selector: app: productpage --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: productpage-v1 spec: replicas: 1 template: metadata: labels: app: productpage version: v1 spec: containers: - name: productpage image: istio/examples-bookinfo-productpage-v1:1.8.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9080 再查看下 productpage 容器的 Dockerfile。 FROM python:2.7-slim COPY requirements.txt ./ RUN pip install --no-cache-dir -r requirements.txt COPY productpage.py /opt/microservices/ COPY templates /opt/microservices/templates COPY requirements.txt /opt/microservices/ EXPOSE 9080 WORKDIR /opt/microservices CMD python productpage.py 9080 我们看到 Dockerfile 中没有配置 ENTRYPOINT，所以 CMD 的配置 python productpage.py 9080 将作为默认的 ENTRYPOINT，记住这一点，再看下注入 sidecar 之后的配置。 $ istioctl kube-inject -f yaml/istio-bookinfo/bookinfo.yaml 我们只截取其中与 productpage 相关的 Service 和 Deployment 配置部分。 apiVersion: v1 kind: Service metadata: name: productpage labels: app: productpage spec: ports: - port: 9080 name: http selector: app: productpage --- apiVersion: extensions/v1beta1 kind: Deployment metadata: creationTimestamp: null name: productpage-v1 spec: replicas: 1 strategy: {} template: metadata: annotations: sidecar.istio.io/status: '{\"version\":\"fde14299e2ae804b95be08e0f2d171d466f47983391c00519bbf01392d9ad6bb\",\"initContainers\":[\"istio-init\"],\"containers\":[\"istio-proxy\"],\"volumes\":[\"istio-envoy\",\"istio-certs\"],\"imagePullSecrets\":null}' creationTimestamp: null labels: app: productpage version: v1 spec: containers: - image: istio/examples-bookinfo-productpage-v1:1.8.0 imagePullPolicy: IfNotPresent name: productpage ports: - containerPort: 9080 resources: {} - args: - proxy - sidecar - --configPath - /etc/istio/proxy - --binaryPath - /usr/local/bin/envoy - --serviceCluster - productpage - --drainDuration - 45s - --parentShutdownDuration - 1m0s - --discoveryAddress - istio-pilot.istio-system:15007 - --discoveryRefreshDelay - 1s - --zipkinAddress - zipkin.istio-system:9411 - --connectTimeout - 10s - --statsdUdpAddress - istio-statsd-prom-bridge.istio-system:9125 - --proxyAdminPort - \"15000\" - --controlPlaneAuthPolicy - NONE env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: INSTANCE_IP valueFrom: fieldRef: fieldPath: status.podIP - name: ISTIO_META_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: ISTIO_META_INTERCEPTION_MODE value: REDIRECT image: jimmysong/istio-release-proxyv2:1.0.0 imagePullPolicy: IfNotPresent name: istio-proxy resources: requests: cpu: 10m securityContext: privileged: false readOnlyRootFilesystem: true runAsUser: 1337 volumeMounts: - mountPath: /etc/istio/proxy name: istio-envoy - mountPath: /etc/certs/ name: istio-certs readOnly: true initContainers: - args: - -p - \"15001\" - -u - \"1337\" - -m - REDIRECT - -i - '*' - -x - \"\" - -b - 9080, - -d - \"\" image: jimmysong/istio-release-proxy_init:1.0.0 imagePullPolicy: IfNotPresent name: istio-init resources: {} securityContext: capabilities: add: - NET_ADMIN privileged: true volumes: - emptyDir: medium: Memory name: istio-envoy - name: istio-certs secret: optional: true secretName: istio.default status: {} 我们看到 Service 的配置没有变化，所有的变化都在 Deployment 里，Istio 给应用 Pod 注入的配置主要包括： Init 容器 istio-init：用于给 Sidecar 容器即 Envoy 代理做初始化，设置 iptables 端口转发 Envoy sidecar 容器 istio-proxy：运行 Envoy 代理 接下来将分别解析下这两个容器。 Init 容器解析 Istio 在 Pod 中注入的 Init 容器名为 istio-init，我们在上面 Istio 注入完成后的 YAML 文件中看到了该容器的启动参数： -p 15001 -u 1337 -m REDIRECT -i '*' -x \"\" -b 9080 -d \"\" 我们再检查下该容器的 Dockerfile 看看 ENTRYPOINT 是什么以确定启动时执行的命令。 FROM ubuntu:xenial RUN apt-get update && apt-get install -y \\ iproute2 \\ iptables \\ && rm -rf /var/lib/apt/lists/* ADD istio-iptables.sh /usr/local/bin/ ENTRYPOINT [\"/usr/local/bin/istio-iptables.sh\"] 我们看到 istio-init 容器的入口是 /usr/local/bin/istio-iptables.sh 脚本，再按图索骥看看这个脚本里到底写的什么，该脚本的位置在 Istio 源码仓库的 tools/deb/istio-iptables.sh，一共 300 多行，就不贴在这里了。下面我们就来解析下这个启动脚本。 Init 容器启动入口 Init 容器的启动入口是 /usr/local/bin/istio-iptables.sh 脚本，该脚本的用法如下： $ istio-iptables.sh -p PORT -u UID -g GID [-m mode] [-b ports] [-d ports] [-i CIDR] [-x CIDR] [-h] -p: 指定重定向所有 TCP 流量的 Envoy 端口（默认为 $ENVOY_PORT = 15001） -u: 指定未应用重定向的用户的 UID。通常，这是代理容器的 UID（默认为 $ENVOY_USER 的 uid，istio_proxy 的 uid 或 1337） -g: 指定未应用重定向的用户的 GID。（与 -u param 相同的默认值） -m: 指定入站连接重定向到 Envoy 的模式，“REDIRECT” 或 “TPROXY”（默认为 $ISTIO_INBOUND_INTERCEPTION_MODE) -b: 逗号分隔的入站端口列表，其流量将重定向到 Envoy（可选）。使用通配符 “*” 表示重定向所有端口。为空时表示禁用所有入站重定向（默认为 $ISTIO_INBOUND_PORTS） -d: 指定要从重定向到 Envoy 中排除（可选）的入站端口列表，以逗号格式分隔。使用通配符“*” 表示重定向所有入站流量（默认为 $ISTIO_LOCAL_EXCLUDE_PORTS） -i: 指定重定向到 Envoy（可选）的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量。空列表将禁用所有出站重定向（默认为 $ISTIO_SERVICE_CIDR） -x: 指定将从重定向中排除的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量（默认为 $ISTIO_SERVICE_EXCLUDE_CIDR）。 环境变量位于 $ISTIO_SIDECAR_CONFIG（默认在：/var/lib/istio/envoy/sidecar.env） 通过查看该脚本你将看到，以上传入的参数都会重新组装成 iptables 命令的参数。 再参考 istio-init 容器的启动参数，完整的启动命令如下： $ /usr/local/bin/istio-iptables.sh -p 15001 -u 1337 -m REDIRECT -i '*' -x \"\" -b 9080 -d \"\" 该容器存在的意义就是让 Envoy 代理可以拦截所有的进出 Pod 的流量，即将入站流量重定向到 Sidecar，再拦截应用容器的出站流量经过 Sidecar 处理后再出站。 命令解析 这条启动命令的作用是： 将应用容器的所有流量都转发到 Envoy 的 15001 端口。 使用 istio-proxy 用户身份运行， UID 为 1337，即 Envoy 所处的用户空间，这也是 istio-proxy 容器默认使用的用户，见 YAML 配置中的 runAsUser 字段。 使用默认的 REDIRECT 模式来重定向流量。 将所有出站流量都重定向到 Envoy 代理。 将所有访问 9080 端口（即应用容器 productpage 的端口）的流量重定向到 Envoy 代理。 因为 Init 容器初始化完毕后就会自动终止，因为我们无法登陆到容器中查看 iptables 信息，但是 Init 容器初始化结果会保留到应用容器和 Sidecar 容器中。 istio-proxy 容器解析 为了查看 iptables 配置，我们需要登陆到 Sidecar 容器中使用 root 用户来查看，因为 kubectl 无法使用特权模式来远程操作 docker 容器，所以我们需要登陆到 productpage Pod 所在的主机上使用 docker 命令登陆容器中查看。 查看 productpage Pod 所在的主机。 $ kubectl -n default get pod -l app=productpage -o wide NAME READY STATUS RESTARTS AGE IP NODE productpage-v1-745ffc55b7-2l2lw 2/2 Running 0 1d 172.33.78.10 node3 从输出结果中可以看到该 Pod 运行在 node3 上，使用 vagrant 命令登陆到 node3 主机中并切换为 root 用户。 $ vagrant ssh node3 $ sudo -i 查看 iptables 配置，列出 NAT（网络地址转换）表的所有规则，因为在 Init 容器启动的时候选择给 istio-iptables.sh 传递的参数中指定将入站流量重定向到 Envoy 的模式为 “REDIRECT”，因此在 iptables 中将只有 NAT 表的规格配置，如果选择 TPROXY 还会有 mangle 表配置。iptables 命令的详细用法请参考 iptables，规则配置请参考 iptables 规则配置。 理解 iptables iptables 是 Linux 内核中的防火墙软件 netfilter 的管理工具，位于用户空间，同时也是 netfilter 的一部分。Netfilter 位于内核空间，不仅有网络地址转换的功能，也具备数据包内容修改、以及数据包过滤等防火墙功能。 在了解 Init 容器初始化的 iptables 之前，我们先来了解下 iptables 和规则配置。 下图展示了 iptables 调用链。 图片 - iptables 调用链 iptables 中的表 Init 容器中使用的的 iptables 版本是 v1.6.0，共包含 5 张表： raw 用于配置数据包，raw 中的数据包不会被系统跟踪。 filter 是用于存放所有与防火墙相关操作的默认表。 nat 用于 网络地址转换（例如：端口转发）。 mangle 用于对特定数据包的修改（参考损坏数据包）。 security 用于强制访问控制 网络规则。 注：在本示例中只用到了 nat 表。 不同的表中的具有的链类型如下表所示： 规则名称 raw filter nat mangle security PREROUTING ✓ ✓ ✓ INPUT ✓ ✓ ✓ ✓ OUTPUT ✓ ✓ ✓ ✓ POSTROUTING ✓ ✓ FORWARD ✓ ✓ ✓ ✓ 下图是 iptables 的调用链顺序。 图片 - iptables 调用链 关于 iptables 的详细介绍请参考常见 iptables 使用规则场景整理。 iptables 命令 iptables 命令的主要用途是修改这些表中的规则。iptables 命令格式如下： $ iptables [-t 表名] 命令选项［链名]［条件匹配］[-j 目标动作或跳转］ Init 容器中的 /istio-iptables.sh 启动入口脚本就是执行 iptables 初始化的。 理解 iptables 规则 查看 istio-proxy 容器中的默认的 iptables 规则，默认查看的是 filter 表中的规则。 $ iptables -L -v Chain INPUT (policy ACCEPT 350K packets, 63M bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18M packets, 1916M bytes) pkts bytes target prot opt in out source destination 我们看到三个默认的链，分别是 INPUT、FORWARD 和 OUTPUT，每个链中的第一行输出表示链名称（在本例中为INPUT/FORWARD/OUTPUT），后跟默认策略（ACCEPT）。 下图是 iptables 的建议结构图，流量在经过 INPUT 链之后就进入了上层协议栈，比如 图片 - iptables结构图 图片来自常见 iptables 使用规则场景整理 每条链中都可以添加多条规则，规则是按照顺序从前到后执行的。我们来看下规则的表头定义。 pkts：处理过的匹配的报文数量 bytes：累计处理的报文大小（字节数） target：如果报文与规则匹配，指定目标就会被执行。 prot：协议，例如 tdp、udp、icmp 和 all。 opt：很少使用，这一列用于显示 IP 选项。 in：入站网卡。 out：出站网卡。 source：流量的源 IP 地址或子网，后者是 anywhere。 destination：流量的目的地 IP 地址或子网，或者是 anywhere。 还有一列没有表头，显示在最后，表示规则的选项，作为规则的扩展匹配条件，用来补充前面的几列中的配置。prot、opt、in、out、source 和 destination 和显示在 destination 后面的没有表头的一列扩展条件共同组成匹配规则。当流量匹配这些规则后就会执行 target。 关于 iptables 规则请参考常见iptables使用规则场景整理。 target 支持的类型 target 类型包括 ACCEPT、REJECT、DROP、LOG 、SNAT、MASQUERADE、DNAT、REDIRECT、RETURN 或者跳转到其他规则等。只要执行到某一条链中只有按照顺序有一条规则匹配后就可以确定报文的去向了，除了 RETURN 类型，类似编程语言中的 return 语句，返回到它的调用点，继续执行下一条规则。target 支持的配置详解请参考 iptables 详解（1）：iptables 概念。 从输出结果中可以看到 Init 容器没有在 iptables 的默认链路中创建任何规则，而是创建了新的链路。 查看 iptables nat 表中注入的规则 Init 容器通过向 iptables nat 表中注入转发规则来劫持流量的，下图显示的是 productpage 服务中的 iptables 流量劫持的详细过程。 图片 - Envoy sidecar 流量劫持流程示意图 Init 容器启动时命令行参数中指定了 REDIRECT 模式，因此只创建了 NAT 表规则，接下来我们查看下 NAT 表中创建的规则，这是全文中的重点部分，前面讲了那么多都是为它做铺垫的。下面是查看 nat 表中的规则，其中链的名字中包含 ISTIO 前缀的是由 Init 容器注入的，规则匹配是根据下面显示的顺序来执行的，其中会有多次跳转。 # 查看 NAT 表中规则配置的详细信息 $ iptables -t nat -L -v # PREROUTING 链：用于目标地址转换（DNAT），将所有入站 TCP 流量跳转到 ISTIO_INBOUND 链上 Chain PREROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 2 120 ISTIO_INBOUND tcp -- any any anywhere anywhere # INPUT 链：处理输入数据包，非 TCP 流量将继续 OUTPUT 链 Chain INPUT (policy ACCEPT 2 packets, 120 bytes) pkts bytes target prot opt in out source destination # OUTPUT 链：将所有出站数据包跳转到 ISTIO_OUTPUT 链上 Chain OUTPUT (policy ACCEPT 41146 packets, 3845K bytes) pkts bytes target prot opt in out source destination 93 5580 ISTIO_OUTPUT tcp -- any any anywhere anywhere # POSTROUTING 链：所有数据包流出网卡时都要先进入POSTROUTING 链，内核根据数据包目的地判断是否需要转发出去，我们看到此处未做任何处理 Chain POSTROUTING (policy ACCEPT 41199 packets, 3848K bytes) pkts bytes target prot opt in out source destination # ISTIO_INBOUND 链：将所有目的地为 9080 端口的入站流量重定向到 ISTIO_IN_REDIRECT 链上 Chain ISTIO_INBOUND (1 references) pkts bytes target prot opt in out source destination 2 120 ISTIO_IN_REDIRECT tcp -- any any anywhere anywhere tcp dpt:9080 # ISTIO_IN_REDIRECT 链：将所有的入站流量跳转到本地的 15001 端口，至此成功的拦截了流量到 Envoy Chain ISTIO_IN_REDIRECT (1 references) pkts bytes target prot opt in out source destination 2 120 REDIRECT tcp -- any any anywhere anywhere redir ports 15001 # ISTIO_OUTPUT 链：选择需要重定向到 Envoy（即本地） 的出站流量，所有非 localhost 的流量全部转发到 ISTIO_REDIRECT。为了避免流量在该 Pod 中无限循环，所有到 istio-proxy 用户空间的流量都返回到它的调用点中的下一条规则，本例中即 OUTPUT 链，因为跳出 ISTIO_OUTPUT 规则之后就进入下一条链 POSTROUTING。如果目的地非 localhost 就跳转到 ISTIO_REDIRECT；如果流量是来自 istio-proxy 用户空间的，那么就跳出该链，返回它的调用链继续执行下一条规则（OUPT 的下一条规则，无需对流量进行处理）；所有的非 istio-proxy 用户空间的目的地是 localhost 的流量就跳转到 ISTIO_REDIRECT Chain ISTIO_OUTPUT (1 references) pkts bytes target prot opt in out source destination 0 0 ISTIO_REDIRECT all -- any lo anywhere !localhost 40 2400 RETURN all -- any any anywhere anywhere owner UID match istio-proxy 0 0 RETURN all -- any any anywhere anywhere owner GID match istio-proxy 0 0 RETURN all -- any any anywhere localhost 53 3180 ISTIO_REDIRECT all -- any any anywhere anywhere # ISTIO_REDIRECT 链：将所有流量重定向到 Envoy（即本地） 的 15001 端口 Chain ISTIO_REDIRECT (2 references) pkts bytes target prot opt in out source destination 53 3180 REDIRECT tcp -- any any anywhere anywhere redir ports 15001 iptables 显示的链的顺序，即流量规则匹配的顺序。其中要特别注意 ISTIO_OUTPUT 链中的规则配置。为了避免流量一直在 Pod 中无限循环，所有到 istio-proxy 用户空间的流量都返回到它的调用点中的下一条规则，本例中即 OUTPUT 链，因为跳出 ISTIO_OUTPUT 规则之后就进入下一条链 POSTROUTING。 ISTIO_OUTPUT 链规则匹配的详细过程如下： 如果目的地非 localhost 就跳转到 ISTIO_REDIRECT 链 所有来自 istio-proxy 用户空间的非 localhost 流量跳转到它的调用点 OUTPUT 继续执行 OUTPUT 链的下一条规则，因为 OUTPUT 链中没有下一条规则了，所以会继续执行 POSTROUTING 链然后跳出 iptables，直接访问目的地 如果流量不是来自 istio-proxy 用户空间，又是对 localhost 的访问，那么就跳出 iptables，直接访问目的地 其它所有情况都跳转到 ISTIO_REDIRECT 链 其实在最后这条规则前还可以增加 IP 地址过滤，让某些 IP 地址段不通过 Envoy 代理。 图片 - istio sidecar iptables 注入 以上 iptables 规则都是 Init 容器启动的时使用 istio-iptables.sh 脚本生成的，详细过程可以查看该脚本。 查看 Envoy 运行状态 首先查看 proxyv2 镜像的 Dockerfile。 FROM istionightly/base_debug ARG proxy_version ARG istio_version # 安装 Envoy ADD envoy /usr/local/bin/envoy # 使用环境变量的方式明文指定 proxy 的版本/功能 ENV ISTIO_META_ISTIO_PROXY_VERSION \"1.1.0\" # 使用环境变量的方式明文指定 proxy 明确的 sha，用于指定版本的配置和调试 ENV ISTIO_META_ISTIO_PROXY_SHA $proxy_version # 环境变量，指定明确的构建号，用于调试 ENV ISTIO_META_ISTIO_VERSION $istio_version ADD pilot-agent /usr/local/bin/pilot-agent ADD envoy_pilot.yaml.tmpl /etc/istio/proxy/envoy_pilot.yaml.tmpl ADD envoy_policy.yaml.tmpl /etc/istio/proxy/envoy_policy.yaml.tmpl ADD envoy_telemetry.yaml.tmpl /etc/istio/proxy/envoy_telemetry.yaml.tmpl ADD istio-iptables.sh /usr/local/bin/istio-iptables.sh COPY envoy_bootstrap_v2.json /var/lib/istio/envoy/envoy_bootstrap_tmpl.json RUN chmod 755 /usr/local/bin/envoy /usr/local/bin/pilot-agent # 将 istio-proxy 用户加入 sudo 权限以允许执行 tcpdump 和其他调试命令 RUN useradd -m --uid 1337 istio-proxy && \\ echo \"istio-proxy ALL=NOPASSWD: ALL\" >> /etc/sudoers && \\ chown -R istio-proxy /var/lib/istio # 使用 pilot-agent 来启动 Envoy ENTRYPOINT [\"/usr/local/bin/pilot-agent\"] 该容器的启动入口是 pilot-agent 命令，根据 YAML 配置中传递的参数，详细的启动命令入下： /usr/local/bin/pilot-agent proxy sidecar --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istio-pilot.istio-system:15007 --discoveryRefreshDelay 1s --zipkinAddress zipkin.istio-system:9411 --connectTimeout 10s --statsdUdpAddress istio-statsd-prom-bridge.istio-system:9125 --proxyAdminPort 15000 --controlPlaneAuthPolicy NONE 主要配置了 Envoy 二进制文件的位置、服务发现地址、服务集群名、监控指标上报地址、Envoy 的管理端口、热重启时间等，详细用法请参考 Istio官方文档 pilot-agent 的用法。 pilot-agent 是容器中 PID 为 1 的启动进程，它启动时又创建了一个 Envoy 进程，如下： /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage --service-node sidecar~172.33.78.10~productpage-v1-745ffc55b7-2l2lw.default~default.svc.cluster.local --max-obj-name-len 189 -l warn --v2-config-only 我们分别解释下以上配置的意义。 -c /etc/istio/proxy/envoy-rev0.json：配置文件，支持 .json、.yaml、.pb 和 .pb_text 格式，pilot-agent 启动的时候读取了容器的环境变量后创建的。 --restart-epoch 0：Envoy 热重启周期，第一次启动默认为 0，每热重启一次该值加 1。 --drain-time-s 45：热重启期间 Envoy 将耗尽连接的时间。 --parent-shutdown-time-s 60： Envoy 在热重启时关闭父进程之前等待的时间。 --service-cluster productpage：Envoy 运行的本地服务集群的名字。 --service-node sidecar~172.33.78.10~productpage-v1-745ffc55b7-2l2lw.default~default.svc.cluster.local：定义 Envoy 运行的本地服务节点名称，其中包含了该 Pod 的名称、IP、DNS 域等信息，根据容器的环境变量拼出来的。 -max-obj-name-len 189：cluster/route_config/listener 中名称字段的最大长度（以字节为单位） -l warn：日志级别 --v2-config-only：只解析 v2 引导配置文件 详细配置请参考 Envoy 的命令行选项。 查看 Envoy 的配置文件 /etc/istio/proxy/envoy-rev0.json。 { \"node\": { \"id\": \"sidecar~172.33.78.10~productpage-v1-745ffc55b7-2l2lw.default~default.svc.cluster.local\", \"cluster\": \"productpage\", \"metadata\": { \"INTERCEPTION_MODE\": \"REDIRECT\", \"ISTIO_PROXY_SHA\": \"istio-proxy:6166ae7ebac7f630206b2fe4e6767516bf198313\", \"ISTIO_PROXY_VERSION\": \"1.0.0\", \"ISTIO_VERSION\": \"1.0.0\", \"POD_NAME\": \"productpage-v1-745ffc55b7-2l2lw\", \"istio\": \"sidecar\" } }, \"stats_config\": { \"use_all_default_tags\": false }, \"admin\": { \"access_log_path\": \"/dev/stdout\", \"address\": { \"socket_address\": { \"address\": \"127.0.0.1\", \"port_value\": 15000 } } }, \"dynamic_resources\": { \"lds_config\": { \"ads\": {} }, \"cds_config\": { \"ads\": {} }, \"ads_config\": { \"api_type\": \"GRPC\", \"refresh_delay\": {\"seconds\": 1, \"nanos\": 0}, \"grpc_services\": [ { \"envoy_grpc\": { \"cluster_name\": \"xds-grpc\" } } ] } }, \"static_resources\": { \"clusters\": [ { \"name\": \"xds-grpc\", \"type\": \"STRICT_DNS\", \"connect_timeout\": {\"seconds\": 10, \"nanos\": 0}, \"lb_policy\": \"ROUND_ROBIN\", \"hosts\": [ { \"socket_address\": {\"address\": \"istio-pilot.istio-system\", \"port_value\": 15010} } ], \"circuit_breakers\": { \"thresholds\": [ { \"priority\": \"default\", \"max_connections\": \"100000\", \"max_pending_requests\": \"100000\", \"max_requests\": \"100000\" }, { \"priority\": \"high\", \"max_connections\": \"100000\", \"max_pending_requests\": \"100000\", \"max_requests\": \"100000\" }] }, \"upstream_connection_options\": { \"tcp_keepalive\": { \"keepalive_time\": 300 } }, \"http2_protocol_options\": { } } , { \"name\": \"zipkin\", \"type\": \"STRICT_DNS\", \"connect_timeout\": { \"seconds\": 1 }, \"lb_policy\": \"ROUND_ROBIN\", \"hosts\": [ { \"socket_address\": {\"address\": \"zipkin.istio-system\", \"port_value\": 9411} } ] } ] }, \"tracing\": { \"http\": { \"name\": \"envoy.zipkin\", \"config\": { \"collector_cluster\": \"zipkin\" } } }, \"stats_sinks\": [ { \"name\": \"envoy.statsd\", \"config\": { \"address\": { \"socket_address\": {\"address\": \"10.254.109.175\", \"port_value\": 9125} } } } ] } 下图是使用 Istio 管理的 bookinfo 示例的访问请求路径图。 图片 - Istio bookinfo 图片来自 Istio 官方网站 对照 bookinfo 示例的 productpage 的查看建立的连接。在 productpage-v1-745ffc55b7-2l2lw Pod 的 istio-proxy 容器中使用 root 用户查看打开的端口。 $ lsof -i COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME envoy 11 istio-proxy 9u IPv4 73951 0t0 TCP localhost:15000 (LISTEN) # Envoy admin 端口 envoy 11 istio-proxy 17u IPv4 74320 0t0 TCP productpage-v1-745ffc55b7-2l2lw:46862->istio-pilot.istio-system.svc.cluster.local:15010 (ESTABLISHED) # 15010：istio-pilot 的 grcp-xds 端口 envoy 11 istio-proxy 18u IPv4 73986 0t0 UDP productpage-v1-745ffc55b7-2l2lw:44332->istio-statsd-prom-bridge.istio-system.svc.cluster.local:9125 # 给 Promethues 发送 metric 的端口 envoy 11 istio-proxy 52u IPv4 74599 0t0 TCP *:15001 (LISTEN) # Envoy 的监听端口 envoy 11 istio-proxy 53u IPv4 74600 0t0 UDP productpage-v1-745ffc55b7-2l2lw:48011->istio-statsd-prom-bridge.istio-system.svc.cluster.local:9125 # 给 Promethues 发送 metric 端口 envoy 11 istio-proxy 54u IPv4 338551 0t0 TCP productpage-v1-745ffc55b7-2l2lw:15001->172.17.8.102:52670 (ESTABLISHED) # 52670：Ingress gateway 端口 envoy 11 istio-proxy 55u IPv4 338364 0t0 TCP productpage-v1-745ffc55b7-2l2lw:44046->172.33.78.9:9091 (ESTABLISHED) # 9091：istio-telemetry 服务的 grpc-mixer 端口 envoy 11 istio-proxy 56u IPv4 338473 0t0 TCP productpage-v1-745ffc55b7-2l2lw:47210->zipkin.istio-system.svc.cluster.local:9411 (ESTABLISHED) # 9411: zipkin 端口 envoy 11 istio-proxy 58u IPv4 338383 0t0 TCP productpage-v1-745ffc55b7-2l2lw:41564->172.33.84.8:9080 (ESTABLISHED) # 9080：details-v1 的 http 端口 envoy 11 istio-proxy 59u IPv4 338390 0t0 TCP productpage-v1-745ffc55b7-2l2lw:54410->172.33.78.5:9080 (ESTABLISHED) # 9080：reivews-v2 的 http 端口 envoy 11 istio-proxy 60u IPv4 338411 0t0 TCP productpage-v1-745ffc55b7-2l2lw:35200->172.33.84.5:9091 (ESTABLISHED) # 9091:istio-telemetry 服务的 grpc-mixer 端口 envoy 11 istio-proxy 62u IPv4 338497 0t0 TCP productpage-v1-745ffc55b7-2l2lw:34402->172.33.84.9:9080 (ESTABLISHED) # reviews-v1 的 http 端口 envoy 11 istio-proxy 63u IPv4 338525 0t0 TCP productpage-v1-745ffc55b7-2l2lw:50592->172.33.71.5:9080 (ESTABLISHED) # reviews-v3 的 http 端口 从输出级过上可以验证 Sidecar 是如何接管流量和与 istio-pilot 通信，及向 Mixer 做遥测数据汇聚的。感兴趣的读者可以再去看看其他几个服务的 istio-proxy 容器中的 iptables 和端口信息。 参考 SOFAMesh & SOFA MOSN—基于Istio构建的用于应对大规模流量的Service Mesh解决方案 - jimmysong.io Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册 - jimmysong.io JSONPath Support - kubernetes.io iptables 命令使用说明 - wangchujiang.com How To List and Delete Iptables Firewall Rules - digitalocean.com 一句一句解说 iptables的详细中文手册 - cnblog.com 常见iptables使用规则场景整理 - aliang.org 理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 - jimmysong.io Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"concepts/istio-sidecar-injector.html":{"url":"concepts/istio-sidecar-injector.html","title":"Sidecar 的自动注入过程详解","keywords":"","body":"Sidecar 的自动注入过程详解 在 Sidecar 注入与流量劫持详解中我只是简单介绍了 Sidecar 注入的步骤，但是没有涉及到具体的 Sidecar 注入流程与细节，这一篇将带大家了解 Istio 为数据平面自动注入 Sidecar 的详细过程。 Sidecar 注入过程 如 Istio 官方文档中对 Istio sidecar 注入的描述，你可以使用 istioctl 命令手动注入 Sidecar，也可以为 Kubernetes 集群自动开启 sidecar 注入，这主要用到了 Kubernetes 的准入控制器中的 webhook，参考 Istio 官网中对 Istio sidecar 注入的描述。 图片 - Sidecar 注入流程图 手动注入 sidecar 与自动注入 sidecar 的区别 不论是手动注入还是自动注入，sidecar 的注入过程有需要遵循如下步骤： Kubernetes 需要了解待注入的 sidecar 所连接的 Istio 集群及其配置； Kubernetes 需要了解待注入的 sidecar 容器本身的配置，如镜像地址、启动参数等； Kubernetes 根据 sidecar 注入模板和以上配置填充 sidecar 的配置参数，将以上配置注入到应用容器的一侧； Istio 和 sidecar 配置保存在 istio 和 istio-sidecar-injector 这两个 ConfigMap 中，其中包含了 Go template，所谓自动 sidecar 注入就是将生成 Pod 配置从应用 YAML 文件期间转移到 mutable webhook 中。 参考 注入 Istio sidecar - istio.io Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"concepts/traffic-management.html":{"url":"concepts/traffic-management.html","title":"流量管理","keywords":"","body":"流量管理 这一章节将大家了解 Istio 流量管理中的各种概念的含义及表示方法。 流量管理是 Isito 中的最基础功能，使用 Istio 的流量管理模型，本质上是将流量与基础设施扩容解耦，让运维人员可以通过 Pilot 指定流量遵循什么规则，而不是指定哪些 pod/VM 应该接收流量——Pilot 和智能 Envoy 代理会帮我们搞定。 所谓流量管理是指： 控制服务之间的路由：通过在 VirtualService 中的规则条件匹配来设置路由，可以在服务间拆分流量。 控制路由上流量的行为：设定好路由之后，就可以在路由上指定超时和重试机制，例如超时时间、重试次数等；做错误注入、设置断路器等。可以由 VirtualService 和 DestinationRule 共同完成。 显式地向网格中注册服务：显示地引入 Service Mesh 内部或外部的服务，纳入服务网格管理。由 ServiceEntry 实现。 控制网格边缘的南北向流量：为了管理进入 Istio service mesh 的南北向入口流量，需要创建 Gateway 对象并与 VirtualService 绑定。 关于流量管理的详细介绍请参考 Istio 官方文档。 参考 流量管理 - istio.io Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"concepts/traffic-management-basic.html":{"url":"concepts/traffic-management-basic.html","title":"流量管理基础概念","keywords":"","body":"流量管理基础概念 下面将带您了解 Istio 流量管理相关的基础概念与配置示例。 VirtualService 在 Istio 服务网格中定义路由规则，控制流量路由到服务上的各种行为。 DestinationRule 是 VirtualService 路由生效后，配置应用与请求的策略集。 ServiceEntry 通常用于在 Istio 服务网格之外启用的服务请求。 Gateway 为 HTTP/TCP 流量配置负载均衡器，最常见的是在网格边缘的操作，以启用应用程序的入口流量。 EnvoyFilter 描述了针对代理服务的过滤器，用来定制由 Istio Pilot 生成的代理配置。一定要谨慎使用此功能。错误的配置内容一旦完成传播，可能会令整个服务网格陷入瘫痪状态。这一配置是用于对 Istio 网络系统内部实现进行变更的。 注：本文中的示例引用自 Istio 官方 Bookinfo 示例，见：Istio 代码库，且对于配置的讲解都以在 Kubernetes 中部署的服务为准。 VirtualService VirtualService 故名思义，就是虚拟服务，在 Istio 1.0 以前叫做 RouteRule。VirtualService 中定义了一系列针对指定服务的流量路由规则。每个路由规则都是针对特定协议的匹配规则。如果流量符合这些特征，就会根据规则发送到服务注册表中的目标服务（或者目标服务的子集或版本）。VirtualService 的详细定义和配置请参考通信路由。 注意：VirtualService 中的规则是按照在 YAML 文件中的顺序执行的，这就是为什么在存在多条规则时，需要慎重考虑优先级的原因。 配置说明 下面是 VirtualService 的配置说明。 字段 类型 描述 hosts string[] 必要字段：流量的目标主机。可以是带有通配符前缀的 DNS 名称，也可以是 IP 地址。根据所在平台情况，还可能使用短名称来代替 FQDN。这种场景下，短名称到 FQDN 的具体转换过程是要靠下层平台完成的。一个主机名只能在一个 VirtualService 中定义。同一个 VirtualService 中可以用于控制多个 HTTP 和 TCP 端口的流量属性。Kubernetes 用户注意：当使用服务的短名称时（例如使用 reviews，而不是 reviews.default.svc.cluster.local），Istio 会根据规则所在的命名空间来处理这一名称，而非服务所在的命名空间。假设 “default” 命名空间的一条规则中包含了一个 reviews 的 host 引用，就会被视为 reviews.default.svc.cluster.local，而不会考虑 reviews 服务所在的命名空间。为了避免可能的错误配置，建议使用 FQDN 来进行服务引用。 hosts 字段对 HTTP 和 TCP 服务都是有效的。网格中的服务也就是在服务注册表中注册的服务，必须使用他们的注册名进行引用；只有 Gateway 定义的服务才可以使用 IP 地址。 gateways string[] Gateway 名称列表，Sidecar 会据此使用路由。VirtualService 对象可以用于网格中的 Sidecar，也可以用于一个或多个 Gateway。这里公开的选择条件可以在协议相关的路由过滤条件中进行覆盖。保留字 mesh 用来指代网格中的所有 Sidecar。当这一字段被省略时，就会使用缺省值（mesh），也就是针对网格中的所有 Sidecar 生效。如果提供了 gateways 字段，这一规则就只会应用到声明的 Gateway 之中。要让规则同时对 Gateway 和网格内服务生效，需要显式的将 mesh 加入 gateways 列表。 http HTTPRoute[] HTTP 流量规则的有序列表。这个列表对名称前缀为 http-、http2-、grpc- 的服务端口，或者协议为 HTTP、HTTP2、GRPC 以及终结的 TLS，另外还有使用 HTTP、HTTP2 以及 GRPC 协议的 ServiceEntry 都是有效的。进入流量会使用匹配到的第一条规则。 tls TLSRoute[] 一个有序列表，对应的是透传 TLS 和 HTTPS 流量。路由过程通常利用 ClientHello 消息中的 SNI 来完成。TLS 路由通常应用在 https-、tls- 前缀的平台服务端口，或者经 Gateway 透传的 HTTPS、TLS 协议端口，以及使用 HTTPS 或者 TLS 协议的 ServiceEntry 端口上。注意：没有关联 VirtualService 的 https- 或者 tls- 端口流量会被视为透传 TCP 流量。 tcp TCPRoute[] 一个针对透传 TCP 流量的有序路由列表。TCP 路由对所有 HTTP 和 TLS 之外的端口生效。进入流量会使用匹配到的第一条规则。 示例 下面的例子中配置了一个名为 reviews 的 VirtualService，该配置的作用是将所有发送给 reviews 服务的流量发送到 v1 版本的子集。 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 该配置中流量的目标主机是 reviews，如果该服务和规则部署在 Kubernetes 的 default namespace 下的话，对应于 Kubernetes 中的服务的 DNS 名称就是 reviews.default.svc.cluster.local。 我们在 hosts 配置了服务的名字只是表示该配置是针对 reviews.default.svc.cluster.local 的服务的路由规则，但是具体将对该服务的访问的流量路由到哪些服务的哪些实例上，就是要通过 destination 的配置了。 我们看到上面的 VirtualService 的 HTTP 路由中还定义了一个 destination。destination 用于定义在网络中可寻址的服务，请求或连接在经过路由规则的处理之后，就会被发送给 destination。destination.host 应该明确指向服务注册表中的一个服务。Istio 的服务注册表除包含平台服务注册表中的所有服务（例如 Kubernetes 服务、Consul 服务）之外，还包含了 ServiceEntry 资源所定义的服务。VirtualService 中只定义流量发送给哪个服务的路由规则，但是并不知道要发送的服务的地址是什么，这就需要 DestinationRule 来定义了。 subset 配置流量目的地的子集，下文会讲到。VirtualService 中其实可以除了 hosts 字段外其他什么都不配置，路由规则可以在 DestinationRule 中单独配置来覆盖此处的默认规则。 Subset subset 不属于 Istio 创建的 CRD，但是它是一条重要的配置信息，有必要单独说明下。subset 是服务端点的集合，可以用于 A/B 测试或者分版本路由等场景。参考 VirtualService 文档，其中会有更多这方面应用的例子。另外在 subset 中可以覆盖服务级别的即 VirtualService 中的定义的流量策略。 以下是subset 的配置信息。对于 Kubernetes 中的服务，一个 subset 相当于使用 label 的匹配条件选出来的 service。 字段 类型 描述 name string 必要字段。服务名和 subset 名称可以用于路由规则中的流量拆分。 labels map 必要字段。使用标签对服务注册表中的服务端点进行筛选。 trafficPolicy TrafficPolicy 应用到这一 subset 的流量策略。缺省情况下 subset 会继承 DestinationRule 级别的策略，这一字段的定义则会覆盖缺省的继承策略。 DestinationRule DestinationRule 所定义的策略，决定了经过路由处理之后的流量的访问策略。这些策略中可以定义负载均衡配置、连接池大小以及外部检测（用于在负载均衡池中对不健康主机进行识别和驱逐）配置。 配置说明 下面是 DestinationRule 的配置说明。 字段 类型 描述 name string 必要字段。服务名和 subset 名称可以用于路由规则中的流量拆分。 labels map 必要字段。使用标签对服务注册表中的服务端点进行筛选。 trafficPolicy TrafficPolicy 应用到这一子集的流量策略。缺省情况下子集会继承 DestinationRule 级别的策略，这一字段的定义则会覆盖缺省的继承策略。 示例 下面是一条对 productpage 服务的流量目的地策略的配置。 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 该路由策略将所有对 reviews 服务的流量路由到 v1 的 subset。 ServiceEntry Istio 服务网格内部会维护一个与平台无关的使用通用模型表示的服务注册表，当你的服务网格需要访问外部服务的时候，就需要使用 ServiceEntry 来添加服务注册。 EnvoyFilter EnvoyFilter 描述了针对代理服务的过滤器，用来定制由 Istio Pilot 生成的代理配置。一定要谨慎使用此功能。错误的配置内容一旦完成传播，可能会令整个服务网格陷入瘫痪状态。这一配置是用于对 Istio 网络系统内部实现进行变更的，属于高级配置，用于扩展 Envoy 中的过滤器的。 Gateway Gateway 为 HTTP/TCP 流量配置了一个负载均衡，多数情况下在网格边缘进行操作，用于启用一个服务的入口（ingress）流量，相当于前端代理。与 Kubernetes 的 Ingress 不同，Istio Gateway 只配置四层到六层的功能（例如开放端口或者 TLS 配置），而 Kubernetes 的 Ingress 是七层的。将 VirtualService 绑定到 Gateway 上，用户就可以使用标准的 Istio 规则来控制进入的 HTTP 和 TCP 流量。 Gateway 设置了一个集群外部流量访问集群中的某些服务的入口，而这些流量究竟如何路由到那些服务上则需要通过配置 VirtualServcie 来绑定。下面仍然以 productpage 这个服务来说明。 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: bookinfo-gateway spec: selector: istio: ingressgateway # 使用默认的控制器 servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - \"*\" gateways: - bookinfo-gateway http: - match: - uri: exact: /productpage - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 上面的例子中 bookinfo 这个 VirtualService 中绑定到了 bookinfo-gateway。bookinfo-gateway 使用了标签选择器选择对应的 Kubernetes pod，即下图中的 pod。 图片 - istio ingress gateway pod 我们再看下 istio-ingressgateway 的 YAML 安装配置。 # Deployment 配置 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: istio-ingressgateway namespace: istio-system labels: app: ingressgateway chart: gateways-1.0.0 release: RELEASE-NAME heritage: Tiller app: istio-ingressgateway istio: ingressgateway spec: replicas: 1 template: metadata: labels: app: istio-ingressgateway istio: ingressgateway annotations: sidecar.istio.io/inject: \"false\" scheduler.alpha.kubernetes.io/critical-pod: \"\" spec: serviceAccountName: istio-ingressgateway-service-account containers: - name: ingressgateway image: \"gcr.io/istio-release/proxyv2:1.0.0\" # 容器启动命令入口是 /usr/local/bin/pilot-agent，后面跟参数 proxy 就会启动一个 Envoy 进程 imagePullPolicy: IfNotPresent ports: - containerPort: 80 - containerPort: 443 - containerPort: 31400 - containerPort: 15011 - containerPort: 8060 - containerPort: 15030 - containerPort: 15031 args: - proxy - router - -v - \"2\" - --discoveryRefreshDelay - '1s' #discoveryRefreshDelay - --drainDuration - '45s' #drainDuration - --parentShutdownDuration - '1m0s' #parentShutdownDuration - --connectTimeout - '10s' #connectTimeout - --serviceCluster - istio-ingressgateway - --zipkinAddress - zipkin:9411 - --statsdUdpAddress - istio-statsd-prom-bridge:9125 - --proxyAdminPort - \"15000\" - --controlPlaneAuthPolicy - NONE - --discoveryAddress - istio-pilot.istio-system:8080 resources: requests: cpu: 10m env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: INSTANCE_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIP - name: ISTIO_META_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name volumeMounts: ... # 服务配置 --- apiVersion: v1 kind: Service metadata: name: istio-ingressgateway namespace: istio-system annotations: labels: chart: gateways-1.0.0 release: RELEASE-NAME heritage: Tiller app: istio-ingressgateway istio: ingressgateway spec: type: NodePort selector: app: istio-ingressgateway istio: ingressgateway ports: - name: http2 # 将 ingressgateway 的 80 端口映射到节点的 31380 端口以代理 HTTP 请求 nodePort: 31380 port: 80 targetPort: 80 - name: https nodePort: 31390 port: 443 - name: tcp nodePort: 31400 port: 31400 - name: tcp-pilot-grpc-tls port: 15011 targetPort: 15011 - name: tcp-citadel-grpc-tls port: 8060 targetPort: 8060 - name: http2-prometheus port: 15030 targetPort: 15030 - name: http2-grafana port: 15031 targetPort: 15031 我们看到 ingressgateway 使用的是 proxyv2 镜像，该镜像容器的启动命令入口是 /usr/local/bin/pilot-agent，后面跟参数 proxy 就会启动一个 Envoy 进程，因此 Envoy 既作为 sidecar 也作为边缘代理，egressgateway 的情况也是类似，只不过它控制的是集群内部对外集群外部的请求。这正好验证了本文开头中所画的 Istio Pilot 架构图。请求 /productpage 、/login、/logout、/api/v1/products 这些 URL 的流量转发给 productpage 服务的 9080 端口，而这些流量进入集群内又是经过 ingressgateway pod 代理的，通过访问 ingressgateway pod 所在的宿主机的 31380 端口进入集群内部的。 示例 我们以官方的 bookinfo 示例来解析流量管理配置。下图是 VirtualService 和 DestinationRule 的示意图，其中只显示了 productpage 和 reviews 服务。 图片 - VirtualSerivce 和 DestimationRule 示意图 在前提条件中我部署了该示例，并列出了该示例中的所有 pod，现在我们使用 istioctl 命令来启动查看 productpage-v1-745ffc55b7-2l2lw pod 中的流量配置。 查看 pod 中 Envoy sidecar 的启动配置信息 Bootstrap 消息是 Envoy 配置的根本来源，Bootstrap 消息的一个关键的概念是静态和动态资源的之间的区别。例如 Listener 或 Cluster 这些资源既可以从 static_resources 静态的获得也可以从 dynamic_resources 中配置的 LDS 或 CDS 之类的 xDS 服务获取。关于 xDS 服务的详解请参考 Envoy 中的 xDS REST 和 gRPC 协议详解。 $ istioctl proxy-config bootstrap productpage-v1-745ffc55b7-2l2lw -o json { \"bootstrap\": { \"node\": { \"id\": \"sidecar~172.33.78.10~productpage-v1-745ffc55b7-2l2lw.default~default.svc.cluster.local\", \"cluster\": \"productpage\", \"metadata\": { \"INTERCEPTION_MODE\": \"REDIRECT\", \"ISTIO_PROXY_SHA\": \"istio-proxy:6166ae7ebac7f630206b2fe4e6767516bf198313\", \"ISTIO_PROXY_VERSION\": \"1.0.0\", \"ISTIO_VERSION\": \"1.0.0\", \"POD_NAME\": \"productpage-v1-745ffc55b7-2l2lw\", \"istio\": \"sidecar\" }, \"buildVersion\": \"0/1.8.0-dev//RELEASE\" }, \"staticResources\": { # Envoy 的静态配置，除非销毁后重设，否则不会改变，配置中会明确指定每个上游主机的已解析网络名称（ IP 地址、端口、unix 域套接字等）。 \"clusters\": [ { \"name\": \"xds-grpc\", \"type\": \"STRICT_DNS\", \"connectTimeout\": \"10.000s\", \"hosts\": [ { # istio-pilot 的地址，指定控制平面地址，这个必须是通过静态的方式配置的 \"socketAddress\": { \"address\": \"istio-pilot.istio-system\", \"portValue\": 15010 } } ], \"circuitBreakers\": { # 断路器配置 \"thresholds\": [ { \"maxConnections\": 100000, \"maxPendingRequests\": 100000, \"maxRequests\": 100000 }, { \"priority\": \"HIGH\", \"maxConnections\": 100000, \"maxPendingRequests\": 100000, \"maxRequests\": 100000 } ] }, \"http2ProtocolOptions\": { }, \"upstreamConnectionOptions\": { # 上游连接选项 \"tcpKeepalive\": { \"keepaliveTime\": 300 } } }, { # zipkin 分布式追踪地址配置 \"name\": \"zipkin\", \"type\": \"STRICT_DNS\", \"connectTimeout\": \"1.000s\", \"hosts\": [ { \"socketAddress\": { \"address\": \"zipkin.istio-system\", \"portValue\": 9411 } } ] } ] }, # 以下是动态配置 \"dynamicResources\": { \"ldsConfig\": { # Listener Discovery Service 配置，直接使用 ADS 配置，此处不用配置 \"ads\": { } }, \"cdsConfig\": { # Cluster Discovery Service 配置，直接使用 ADS 配置，此处不用配置 \"ads\": { } }, \"adsConfig\": { # Aggregated Discovery Service 配置，ADS 中集成了 LDS、RDS、CDS \"apiType\": \"GRPC\", \"grpcServices\": [ { \"envoyGrpc\": { \"clusterName\": \"xds-grpc\" } } ], \"refreshDelay\": \"1.000s\" } }, \"statsSinks\": [ # metric 汇聚的地址 { \"name\": \"envoy.statsd\", \"config\": { \"address\": { \"socket_address\": { \"address\": \"10.254.109.175\", \"port_value\": 9125 } } } } ], \"statsConfig\": { \"useAllDefaultTags\": false }, \"tracing\": { # zipkin 地址 \"http\": { \"name\": \"envoy.zipkin\", \"config\": { \"collector_cluster\": \"zipkin\" } } }, \"admin\": { \"accessLogPath\": \"/dev/stdout\", \"address\": { \"socketAddress\": { \"address\": \"127.0.0.1\", \"portValue\": 15000 } } } }, \"lastUpdated\": \"2018-09-04T03:38:45.645Z\" } 以上为初始信息。 创建一个名为 reviews 的 VirtualService。 $ cat 上面的 VirtualService 定义只是定义了访问 reviews 服务的流量要全部流向 reviews服务的 v1子集，至于哪些实例是 v1 子集，VirtualService 中并没有定义，这就需要再创建个 DestinationRule。 $ cat 同时还可以为每个 subset 设置负载均衡规则。这里面也可以同时创建多个子集，例如同时创建3个 subset 分别对应3个版本的实例。 $ cat 同时配置了三个 subset 当你需要切分流量时可以直接修改 VirtualService 中 destination 里的 subset 即可，还可以根据百分比拆分流量，配置超时和重试，进行错误注入等，详见流量管理 当然上面这个例子中只是简单的将流量全部导到某个 VirtualService 的 subset 中，还可以根据其他限定条件如 HTTP headers、pod 的 label、URL 等。 此时再查询 productpage-v1-745ffc55b7-2l2lw pod 的配置信息。 $ istioctl proxy-config clusters productpage-v1-8d69b45c-bcjqv|grep reviews reviews.default.svc.cluster.local 9080 - outbound EDS reviews.default.svc.cluster.local 9080 v1 outbound EDS reviews.default.svc.cluster.local 9080 v2 outbound EDS reviews.default.svc.cluster.local 9080 v3 outbound EDS 可以看到 reviews 服务的 EDS 设置中包含了3个 subset，另外读者还可以自己运行 istioctl proxy-config listeners 和 istioctl proxy-config route 来查询 pod 的监听器和路由配置。 参考 流量管理 - istio.io 通信路由 - istio.io istioctl 指南 - istio.io Envoy 官方文档中文版 - servicemesher.com Envoy v2 API 概览 - servicemesher.com Envoy 中的 xDS REST 和 gRPC 协议详解 - servicemesher.com Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"concepts/sidecar-traffic-routing-deep-dive.html":{"url":"concepts/sidecar-traffic-routing-deep-dive.html","title":"Istio 中的 Sidecar 的流量路由详解","keywords":"","body":"Istio 中的 Sidecar 的流量路由详解 本文以 Istio 官方的 bookinfo 示例来讲解在进入 Pod 的流量被 iptables 转交给 Envoy sidecar 后，Envoy 是如何做路由转发的，详述了 Inbound 和 Outbound 处理过程。关于流量拦截的详细分析请参考理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持。 下面是 Istio 官方提供的 bookinfo 的请求流程图，假设 bookinfo 应用的所有服务中没有配置 DestinationRule。 图片 - Bookinfo 示例 下面是 Istio 自身组件与 Bookinfo 示例的连接关系图，我们可以看到所有的 HTTP 连接都在 9080 端口监听。 图片 - Bookinfo 示例与 Istio 组件连接关系图 可以在 Google Drive 上下载原图。 Sidecar 注入及流量劫持步骤概述 下面是从 Sidecar 注入、Pod 启动到 Sidecar proxy 拦截流量及 Envoy 处理路由的步骤概览。 1. Kubernetes 通过 Admission Controller 自动注入，或者用户使用 istioctl 命令手动注入 sidecar 容器。 2. 应用 YAML 配置部署应用，此时 Kubernetes API server 接收到的服务创建配置文件中已经包含了 Init 容器及 sidecar proxy。 3. 在 sidecar proxy 容器和应用容器启动之前，首先运行 Init 容器，Init 容器用于设置 iptables（Istio 中默认的流量拦截方式，还可以使用 BPF、IPVS 等方式） 将进入 pod 的流量劫持到 Envoy sidecar proxy。所有 TCP 流量（Envoy 目前只支持 TCP 流量）将被 sidecar 劫持，其他协议的流量将按原来的目的地请求。 4. 启动 Pod 中的 Envoy sidecar proxy 和应用程序容器。这一步的过程请参考通过管理接口获取完整配置。 Sidecar proxy 与应用容器的启动顺序问题 启动 sidecar proxy 和应用容器，究竟哪个容器先启动呢？正常情况是 Envoy Sidecar 和应用程序容器全部启动完成后再开始接收流量请求。但是我们无法预料哪个容器会先启动，那么容器启动顺序是否会对 Envoy 劫持流量有影响呢？答案是肯定的，不过分为以下两种情况。 情况1：应用容器先启动，而 sidecar proxy 仍未就绪 这种情况下，流量被 iptables 转移到 15001 端口，而 Pod 中没有监听该端口，TCP 链接就无法建立，请求失败。 情况2：Sidecar 先启动，请求到达而应用程序仍未就绪 这种情况下请求也肯定会失败，至于是在哪一步开始失败的，留给读者来思考。 问题：如果为 sidecar proxy 和应用程序容器添加就绪和存活探针是否可以解决该问题呢？ 5. 不论是进入还是从 Pod 发出的 TCP 请求都会被 iptables 劫持，inbound 流量被劫持后经 Inbound Handler 处理后转交给应用程序容器处理，outbound 流量被 iptables 劫持后转交给 Outbound Handler 处理，并确定转发的 upstream 和 Endpoint。 6. Sidecar proxy 请求 Pilot 使用 xDS 协议同步 Envoy 配置，其中包括 LDS、EDS、CDS 等，不过为了保证更新的顺序，Envoy 会直接使用 ADS 向 Pilot 请求配置更新。 Envoy 如何处理路由转发 下图展示的是 productpage 服务请求访问 http://reviews.default.svc.cluster.local:9080/，当流量进入 reviews 服务内部时，reviews 服务内部的 Envoy Sidecar 是如何做流量拦截和路由转发的。可以在 Google Drive 上下载原图。 图片 - Envoy sidecar 流量劫持与路由转发示意图 第一步开始时，productpage Pod 中的 Envoy sidecar 已经通过 EDS 选择出了要请求的 reviews 服务的一个 Pod，知晓了其 IP 地址，发送 TCP 连接请求。 Istio 官网中的 Envoy 配置深度解析中是以发起 HTTP 请求的一方来详述 Envoy 做流量转发的过程，而本文中考虑的是接受 downstream 的流量的一方，它既要接收 downstream 发来的请求，自己还需要请求其他服务，例如 reviews 服务中的 Pod 还需要请求 ratings 服务。 reviews 服务有三个版本，每个版本有一个实例，三个版本中的 sidecar 工作步骤类似，下文只以 reviews-v1-cb8655c75-b97zc 这一个 Pod 中的 Sidecar 流量转发步骤来说明。 理解 Inbound Handler Inbound handler 的作用是将 iptables 拦截到的 downstream 的流量转交给 localhost，与 Pod 内的应用程序容器建立连接。 查看下 reviews-v1-cb8655c75-b97zc pod 中的 Listener。 运行 istioctl pc listener reviews-v1-cb8655c75-b97zc 查看该 Pod 中的具有哪些 Listener。 ADDRESS PORT TYPE 172.33.3.3 9080 HTTP 当来自 productpage 的流量抵达 reviews Pod 的时候已经，downstream 必须明确知道 Pod 的 IP 地址为 172.33.3.3 所以才会访问该 Pod，所以该请求是 172.33.3.3:9080。 virtual Listener 从该 Pod 的 Listener 列表中可以看到，0.0.0.0:15001/TCP 的 Listener（其实际名字是 virtual）监听所有的 Inbound 流量，下面是该 Listener 的详细配置。 { \"name\": \"virtual\", \"address\": { \"socketAddress\": { \"address\": \"0.0.0.0\", \"portValue\": 15001 } }, \"filterChains\": [ { \"filters\": [ { \"name\": \"envoy.tcp_proxy\", \"config\": { \"cluster\": \"BlackHoleCluster\", \"stat_prefix\": \"BlackHoleCluster\" } } ] } ], \"useOriginalDst\": true } UseOriginalDst：从配置中可以看出 useOriginalDst 配置指定为 true，这是一个布尔值，缺省为 false，使用 iptables 重定向连接时，proxy 接收的端口可能与原始目的地址的端口不一样，如此处 proxy 接收的端口为 15001，而原始目的地端口为 9080。当此标志设置为 true 时，Listener 将连接重定向到与原始目的地址关联的 Listener，此处为 172.33.3.3:9080。如果没有与原始目的地址关联的 Listener，则连接由接收它的 Listener 处理，即该 virtual Listener，经过 envoy.tcp_proxy 过滤器处理转发给 BlackHoleCluster，这个 Cluster 的作用正如它的名字，当 Envoy 找不到匹配的虚拟监听器时，就会将请求发送给它，并返回 404。这个将于下文提到的 Listener 中设置 bindToPort 相呼应。 注意：该参数将被废弃，请使用原始目的地址的 Listener filter 替代。该参数的主要用途是：Envoy 通过监听 15001 端口将 iptables 拦截的流量经由其他 Listener 处理而不是直接转发出去，详情见 Virtual Listener。 Listener 172.33.3.3_9080 上文说到进入 Inbound handler 的流量被 virtual Listener 转移到 172.33.3.3_9080 Listener，我们在查看下该 Listener 配置。 运行 istioctl pc listener reviews-v1-cb8655c75-b97zc --address 172.33.3.3 --port 9080 -o json 查看。 [{ \"name\": \"172.33.3.3_9080\", \"address\": { \"socketAddress\": { \"address\": \"172.33.3.3\", \"portValue\": 9080 } }, \"filterChains\": [ { \"filterChainMatch\": { \"transportProtocol\": \"raw_buffer\" }, \"filters\": [ { \"name\": \"envoy.http_connection_manager\", \"config\": { ... \"route_config\": { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"validate_clusters\": false, \"virtual_hosts\": [ { \"domains\": [ \"*\" ], \"name\": \"inbound|http|9080\", \"routes\": [ { ... \"route\": { \"cluster\": \"inbound|9080||reviews.default.svc.cluster.local\", \"max_grpc_timeout\": \"0.000s\", \"timeout\": \"0.000s\" } } ] } ] }, \"use_remote_address\": false, ... } } ]， \"deprecatedV1\": { \"bindToPort\": false } ... }, { \"filterChainMatch\": { \"transportProtocol\": \"tls\" }, \"tlsContext\": {... }, \"filters\": [... ] } ], ... }] bindToPort：注意其中有一个 bindToPort 的配置，其值为 false，该配置的缺省值为 true，表示将 Listener 绑定到端口上，此处设置为 false 则该 Listener 只能处理其他 Listener 转移过来的流量，即上文所说的 virtual Listener，我们看其中的 filterChains.filters 中的 envoy.http_connection_manager 配置部分： \"route_config\": { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"validate_clusters\": false, \"virtual_hosts\": [ { \"domains\": [ \"*\" ], \"name\": \"inbound|http|9080\", \"routes\": [ { ... \"route\": { \"cluster\": \"inbound|9080||reviews.default.svc.cluster.local\", \"max_grpc_timeout\": \"0.000s\", \"timeout\": \"0.000s\" } } ] } ] } 该配置表示流量将转交给 Cluster inbound|9080||reviews.default.svc.cluster.local 处理。 Cluster inbound|9080||reviews.default.svc.cluster.local 运行 istioctl pc cluster reviews-v1-cb8655c75-b97zc --fqdn reviews.default.svc.cluster.local --direction inbound -o json 查看该 Cluster 的配置如下。 [ { \"name\": \"inbound|9080||reviews.default.svc.cluster.local\", \"connectTimeout\": \"1.000s\", \"hosts\": [ { \"socketAddress\": { \"address\": \"127.0.0.1\", \"portValue\": 9080 } } ], \"circuitBreakers\": { \"thresholds\": [ {} ] } } ] 可以看到该 Cluster 的 Endpoint 直接对应的就是 localhost，再经过 iptables 转发流量就被应用程序容器消费了。 理解 Outbound Handler 因为 reviews 会向 ratings 服务发送 HTTP 请求，请求的地址是：http://ratings.default.svc.cluster.local:9080/，Outbound handler 的作用是将 iptables 拦截到的本地应用程序发出的流量，经由 Envoy 判断如何路由到 upstream。 应用程序容器发出的请求为 Outbound 流量，被 iptables 劫持后转移给 Envoy Outbound handler 处理，然后经过 virtual Listener、0.0.0.0_9080 Listener，然后通过 Route 9080 找到 upstream 的 cluster，进而通过 EDS 找到 Endpoint 执行路由动作。这一部分可以参考 Istio 官网中的 Envoy 深度配置解析。 Route 9080 reviews 会请求 ratings 服务，运行 istioctl proxy-config routes reviews-v1-cb8655c75-b97zc --name 9080 -o json 查看 route 配置，因为 Envoy 会根据 HTTP header 中的 domains 来匹配 VirtualHost，所以下面只列举了 ratings.default.svc.cluster.local:9080 这一个 VirtualHost。 [{ \"name\": \"ratings.default.svc.cluster.local:9080\", \"domains\": [ \"ratings.default.svc.cluster.local\", \"ratings.default.svc.cluster.local:9080\", \"ratings\", \"ratings:9080\", \"ratings.default.svc.cluster\", \"ratings.default.svc.cluster:9080\", \"ratings.default.svc\", \"ratings.default.svc:9080\", \"ratings.default\", \"ratings.default:9080\", \"10.254.234.130\", \"10.254.234.130:9080\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||ratings.default.svc.cluster.local\", \"timeout\": \"0.000s\", \"maxGrpcTimeout\": \"0.000s\" }, \"decorator\": { \"operation\": \"ratings.default.svc.cluster.local:9080/*\" }, \"perFilterConfig\": {... } } ] }, ..] 从该 Virtual Host 配置中可以看到将流量路由到 Cluster outbound|9080||ratings.default.svc.cluster.local。 Endpoint outbound|9080||ratings.default.svc.cluster.local Istio 1.1 以前版本不支持使用 istioctl 命令直接查询 Cluster 的 Endpoint，可以使用查询 Pilot 的 debug 端点的方式折中。 kubectl exec reviews-v1-cb8655c75-b97zc -c istio-proxy curl http://istio-pilot.istio-system.svc.cluster.local:9093/debug/edsz > endpoints.json endpoints.json 文件中包含了所有 Cluster 的 Endpoint 信息，我们只选取其中的 outbound|9080||ratings.default.svc.cluster.local Cluster 的结果如下。 { \"clusterName\": \"outbound|9080||ratings.default.svc.cluster.local\", \"endpoints\": [ { \"locality\": { }, \"lbEndpoints\": [ { \"endpoint\": { \"address\": { \"socketAddress\": { \"address\": \"172.33.100.2\", \"portValue\": 9080 } } }, \"metadata\": { \"filterMetadata\": { \"istio\": { \"uid\": \"kubernetes://ratings-v1-8558d4458d-ns6lk.default\" } } } } ] } ] } Endpoint 可以是一个或多个，Envoy 将根据一定规则选择适当的 Endpoint 来路由。 注：Istio 1.1 将支持 istioctl pc endpoint 命令来查询 Endpoint。 参考 调试 Envoy 和 Pilot - istio.io 理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持 - jimmysong.io Istio流量管理实现机制深度解析 - zhaohuabing.com Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/envoy-terminology.html":{"url":"data-plane/envoy-terminology.html","title":"Envoy 中的基本术语","keywords":"","body":"Envoy 中的基本术语 Host：能够进行网络通信的实体（在手机或服务器等上的应用程序）。在 Envoy 中主机是指逻辑网络应用程序。只要每台主机都可以独立寻址，一块物理硬件上就运行多个主机。 Downstream：下游（downstream）主机连接到 Envoy，发送请求并或获得响应。 Upstream：上游（upstream）主机获取来自 Envoy 的链接请求和响应。 Cluster: 集群（cluster）是 Envoy 连接到的一组逻辑上相似的上游主机。Envoy 通过服务发现发现集群中的成员。Envoy 可以通过主动运行状况检查来确定集群成员的健康状况。Envoy 如何将请求路由到集群成员由负载均衡策略确定。 Mesh：一组互相协调以提供一致网络拓扑的主机。Envoy mesh 是指一组 Envoy 代理，它们构成了由多种不同服务和应用程序平台组成的分布式系统的消息传递基础。 运行时配置：与 Envoy 一起部署的带外实时配置系统。可以在无需重启 Envoy 或 更改 Envoy 主配置的情况下，通过更改设置来影响操作。 Listener: 监听器（listener）是可以由下游客户端连接的命名网络位置（例如，端口、unix域套接字等）。Envoy 公开一个或多个下游主机连接的侦听器。一般是每台主机运行一个 Envoy，使用单进程运行，但是每个进程中可以启动任意数量的 Listener（监听器），目前只监听 TCP，每个监听器都独立配置一定数量的（L3/L4）网络过滤器。Listenter 也可以通过 Listener Discovery Service（LDS）动态获取。 Listener filter：Listener 使用 listener filter（监听器过滤器）来操作链接的元数据。它的作用是在不更改 Envoy 的核心功能的情况下添加更多的集成功能。Listener filter 的 API 相对简单，因为这些过滤器最终是在新接受的套接字上运行。在链中可以互相衔接以支持更复杂的场景，例如调用速率限制。Envoy 已经包含了多个监听器过滤器。 Http Route Table：HTTP 的路由规则，例如请求的域名，Path 符合什么规则，转发给哪个 Cluster。 Health checking：健康检查会与SDS服务发现配合使用。但是，即使使用其他服务发现方式，也有相应需要进行主动健康检查的情况。详见 health checking。 参考 Envoy 的架构与基本术语 - jimmysong.io Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/istio-sidecar-proxy-config.html":{"url":"data-plane/istio-sidecar-proxy-config.html","title":"Istio sidecar proxy 配置","keywords":"","body":"Istio sidecar proxy 配置 假如您使用 kubernetes-vagrant-centos-cluster 部署了 Kubernetes 集群并开启了 Istio Service Mesh，再部署 bookinfo 示例，那么在 default 命名空间下有一个名字类似于 ratings-v1-7c9949d479-dwkr4 的 Pod，使用下面的命令查看该 Pod 的 Envoy sidecar 的全量配置： kubectl -n default exec ratings-v1-7c9949d479-dwkr4 -c istio-proxy curl http://localhost:15000/config_dump > dump-rating.json 将 Envoy 的运行时配置 dump 出来之后你将看到一个长 6000 余行的配置文件。关于该配置文件的介绍请参考 Envoy v2 API 概览。 下图展示的是 Enovy 的配置。 图片 - Envoy 配置 Istio 会在为 Service Mesh 中的每个 Pod 注入 Sidecar 的时候同时为 Envoy 注入 Bootstrap 配置，其余的配置是通过 Pilot 下发的，注意整个数据平面即 Service Mesh 中的 Envoy 的动态配置应该是相同的。您也可以使用上面的命令检查其他 sidecar 的 Envoy 配置是否跟最上面的那个相同。 使用下面的命令检查 Service Mesh 中的所有有 Sidecar 注入的 Pod 中的 proxy 配置是否同步。 $ istioctl proxy-status PROXY CDS LDS EDS RDS PILOT VERSION details-v1-876bf485f-sx7df.default SYNCED SYNCED SYNCED (100%) SYNCED istio-pilot-5bf6d97f79-6lz4x 1.0.0 ... istioctl 这个命令行工具就像 kubectl 一样有很多神器的魔法，通过它可以高效的管理 Istio 和 debug。 参考 kubernetes-vagrant-centos-cluster - github.com Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/envoy-proxy-config-deep-dive.html":{"url":"data-plane/envoy-proxy-config-deep-dive.html","title":"Envoy proxy 配置详解","keywords":"","body":"Envoy proxy 配置详解 Istio envoy sidecar proxy 配置中包含以下四个部分。 bootstrap：Envoy proxy 启动时候加载的静态配置。 listeners：监听器配置，使用 LDS 下发。 clusters：集群配置，静态配置中包括 xds-grpc 和 zipkin 地址，动态配置使用 CDS 下发。 routes：路由配置，静态配置中包括了本地监听的服务的集群信息，其中引用了 cluster，动态配置使用 RDS 下发。 每个部分中都包含静态配置与动态配置，其中 bootstrap 配置又是在集群启动的时候通过 sidecar 启动参数注入的，配置文件在 /etc/istio/proxy/envoy-rev0.json。 Enovy 的配置 dump 出来后的结构如下图所示。 图片 - Envoy 配置 由于 bootstrap 中的配置是来自 Envoy 启动时加载的静态文件，主要配置了节点信息、tracing、admin 和统计信息收集等信息，这不是本文的重点，大家可以自行研究。 图片 - bootstrap 配置 上图是 bootstrap 的配置信息。 Bootstrap 是 Envoy 中配置的根本来源，Bootstrap 消息中有一个关键的概念，就是静态和动态资源的之间的区别。例如 Listener 或 Cluster 这些资源既可以从 static_resources 静态的获得也可以从 dynamic_resources 中配置的 LDS 或 CDS 之类的 xDS 服务获取。 Listener Listener 顾名思义，就是监听器，监听 IP 地址和端口，然后根据策略转发。 Listener 的特点 每个 Envoy 进程中可以有多个 Listener，Envoy 与 Listener 之间是一对多的关系。 每个 Listener 中可以配置一条 filter 链表（filter_chains），Envoy 会根据 filter 顺序执行过滤。 Listener 可以监听下游的端口，也可以接收来自其他 listener 的数据，形成链式处理。 filter 是可扩展的。 可以静态配置，也可以使用 LDS 动态配置。 目前只能监听 TCP，UDP 还未支持。 Listener 的数据结构 Listener 的数据结构如下，除了 name、address 和 filter_chains 为必须配置之外，其他都为可选的。 { \"name\": \"...\", \"address\": \"{...}\", \"filter_chains\": [], \"use_original_dst\": \"{...}\", \"per_connection_buffer_limit_bytes\": \"{...}\", \"metadata\": \"{...}\", \"drain_type\": \"...\", \"listener_filters\": [], \"transparent\": \"{...}\", \"freebind\": \"{...}\", \"socket_options\": [], \"tcp_fast_open_queue_length\": \"{...}\", \"bugfix_reverse_write_filter_order\": \"{...}\" } 下面是关于上述数据结构中的常用配置解析。 name：该 listener 的 UUID，唯一限定名，默认60个字符，例如 10.254.74.159_15011，可以使用命令参数指定长度限制。 address：监听的逻辑/物理地址和端口号，例如 \"address\": { \"socket_address\": { \"address\": \"10.254.74.159\", \"port_value\": 15011 } } filter_chains：这是一个列表，Envoy 中内置了一些通用的 filter，每种 filter 都有特定的数据结构，Enovy 会根据该配置顺序执行 filter。Envoy 中内置的 filter 有：envoy.client_ssl_auth、envoy.echo、enovy.http_connection_manager、envoy.mongo_proxy、envoy.rate_limit、enovy.redis_proxy、envoy.tcp_proxy、http_filters、thrift_filters等。这些 filter 可以单独使用也可以组合使用，还可以自定义扩展，例如使用 Istio 中的 EnvoyFilter 配置。 use_original_dst：这是一个布尔值，如果使用 iptables 重定向连接，则代理接收的端口可能与原始目的地址的端口不一样。当此标志设置为 true 时，Listener 将重定向的连接切换到与原始目的地址关联的 Listener。如果没有与原始目的地址关联的 Listener，则连接由接收它的 Listener 处理。默认为 false。注意：该参数将被废弃，请使用原始目的地址的 Listener filter 替代。该参数的主要用途是，Envoy 通过监听 15001 端口将应用的流量截取后再由其他 Listener 处理而不是直接转发出去，详情见 Virtual Listener。 关于 Listener 的详细介绍请参考 Envoy v2 API reference - listener。 Cluster Cluster 是指 Envoy 连接的一组逻辑相同的上游主机。Envoy 通过服务发现来发现 cluster 的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到 cluster 的哪个成员。 Cluster 的特点 一组逻辑上相同的主机构成一个 cluster。 可以在 cluster 中定义各种负载均衡策略。 新加入的 cluster 需要一个热身的过程才可以给路由引用，该过程是原子的，即在 cluster 热身之前对于 Envoy 及 Service Mesh 的其余部分来说是不可见的。 可以通过多种方式来配置 cluster，例如静态类型、严格限定 DNS、逻辑 DNS、EDS 等。 Cluster 的数据结构 Cluster 的数据结构如下，除了 name 字段，其他都是可选的。 { \"name\": \"...\", \"alt_stat_name\": \"...\", \"type\": \"...\", \"eds_cluster_config\": \"{...}\", \"connect_timeout\": \"{...}\", \"per_connection_buffer_limit_bytes\": \"{...}\", \"lb_policy\": \"...\", \"hosts\": [], \"load_assignment\": \"{...}\", \"health_checks\": [], \"max_requests_per_connection\": \"{...}\", \"circuit_breakers\": \"{...}\", \"tls_context\": \"{...}\", \"common_http_protocol_options\": \"{...}\", \"http_protocol_options\": \"{...}\", \"http2_protocol_options\": \"{...}\", \"extension_protocol_options\": \"{...}\", \"dns_refresh_rate\": \"{...}\", \"dns_lookup_family\": \"...\", \"dns_resolvers\": [], \"outlier_detection\": \"{...}\", \"cleanup_interval\": \"{...}\", \"upstream_bind_config\": \"{...}\", \"lb_subset_config\": \"{...}\", \"ring_hash_lb_config\": \"{...}\", \"original_dst_lb_config\": \"{...}\", \"least_request_lb_config\": \"{...}\", \"common_lb_config\": \"{...}\", \"transport_socket\": \"{...}\", \"metadata\": \"{...}\", \"protocol_selection\": \"...\", \"upstream_connection_options\": \"{...}\", \"close_connections_on_host_health_failure\": \"...\", \"drain_connections_on_host_removal\": \"...\" } 下面是关于上述数据结构中的常用配置解析。 name：如果你留意到作为 Sidecar 启动的 Envoy 的参数的会注意到 --max-obj-name-len 189，该选项用来用来指定 cluster 的名字，例如 inbound|9080||ratings.default.svc.cluster.local。该名字字符串由 | 分隔成四个部分，分别是 inbound 或 outbound 代表入向流量或出向流量、端口号、subcluster 名称、FQDN，其中 subcluster 名称将对应于 Istio DestinationRule 中配置的 subnet，如果是按照多版本按比例路由的话，该值可以是版本号。 type：即服务发现类型，支持的参数有 STATIC（缺省值）、STRICT_DNS、LOGICAL_DNS、EDS、ORIGINAL_DST。 hosts：这是个列表，配置负载均衡的 IP 地址和端口，只有使用了 STATIC、STRICT_DNS、LOGICAL_DNS 服务发现类型时才需要配置。 eds_cluster_config：如果使用 EDS 做服务发现，则需要配置该项目，其中包括的配置有 service_name 和 ads。 关于 Cluster 的详细介绍请参考 Envoy v2 API reference - cluster。 Route 我们在这里所说的路由指的是 HTTP 路由，这也使得 Envoy 可以用来处理网格边缘的流量。HTTP 路由转发是通过路由过滤器实现的。该过滤器的主要职能就是执行路由表中的指令。除了可以做重定向和转发，路由过滤器还需要处理重试、统计之类的任务。 HTTP 路由的特点 前缀和精确路径匹配规则。 可跨越多个上游集群进行基于权重/百分比的路由。 基于优先级的路由。 基于哈希策略的路由。 Route 的数据结构 { \"name\": \"...\", \"virtual_hosts\": [], \"internal_only_headers\": [], \"response_headers_to_add\": [], \"response_headers_to_remove\": [], \"request_headers_to_add\": [], \"request_headers_to_remove\": [], \"validate_clusters\": \"{...}\" } 下面是关于上述数据结构中的常用配置解析。 name：该名字跟 envoy.http_connection_manager filter 中的 http_filters.rds.route_config_name 一致，在 Istio Service Mesh 中为 Envoy 下发的配置中的 Route 是以监听的端口号作为名字，而同一个名字下面的 virtual_hosts 可以有多个值（数组形式）。 virtual_hosts：因为 VirtualHosts 是 Envoy 中引入的一个重要概念，我们在下文将详细说明 virtual_hosts 的数据结构。 validate_clusters：这是一个布尔值，用来设置开启使用 cluster manager 来检测路由表引用的 cluster 是否有效。如果是路由表是通过 route_config 静态配置的则该值默认设置为 true，如果是使用 rds 动态配置的话，则该值默认设置为 false。 关于 Route 的详细介绍请参考 Envoy v2 API reference - HTTP route configuration。 route.VirtualHost VirtualHost 即上文中 Route 配置中的 virtual_hosts，VirtualHost 是路由配置中的顶级元素。每个虚拟主机都有一个逻辑名称以及一组根据传入请求的 host header 路由到它的域。这允许单个 Listener 为多个顶级域路径树提供服务。基于域选择了虚拟主机后 Envoy 就会处理路由以查看要路由到哪个上游集群或是否执行重定向。 VirtualHost 的数据结构 下面是 VirtualHost 的数据结构，除了 name 和 domains 是必须配置项外，其他皆为可选项。 { \"name\": \"...\", \"domains\": [], \"routes\": [], \"require_tls\": \"...\", \"virtual_clusters\": [], \"rate_limits\": [], \"request_headers_to_add\": [], \"request_headers_to_remove\": [], \"response_headers_to_add\": [], \"response_headers_to_remove\": [], \"cors\": \"{...}\", \"per_filter_config\": \"{...}\", \"include_request_attempt_count\": \"...\" } 下面是关于上述数据结构中的常用配置解析。 name：该 VirtualHost 的名字，一般是 FQDN 加端口，如 details.default.svc.cluster.local:9080。 domains：这是个用来匹配 VirtualHost 的域名（host/authority header）列表，也可以使用通配符，但是通配符不能匹配空字符，除了仅使用 * 作为 domains，注意列表中的值不能重复和存在交集，只要有一条 domain 被匹配上了，就会执行路由。Istio 会为该值配置所有地址解析形式，包括 IP 地址、FQDN 和短域名等。 routes：针对入口流量的有序路由列表，第一个匹配上的路由将被执行。我们在下文将详细说明 route 的数据结构。 下面是一个实际的 VirtualHost 的例子，该配置来自 Bookinfo 应用的 details 应用的 Sidecar 服务。 { \"name\": \"details.default.svc.cluster.local:9080\", \"domains\": [ \"details.default.svc.cluster.local\", \"details.default.svc.cluster.local:9080\", \"details\", \"details:9080\", \"details.default.svc.cluster\", \"details.default.svc.cluster:9080\", \"details.default.svc\", \"details.default.svc:9080\", \"details.default\", \"details.default:9080\", \"10.254.4.113\", \"10.254.4.113:9080\" ], \"routes\": [ { \"match\": { \"prefix\": \"/\" }, \"route\": { \"cluster\": \"outbound|9080||details.default.svc.cluster.local\", \"timeout\": \"0s\", \"max_grpc_timeout\": \"0s\" }, \"decorator\": { \"operation\": \"details.default.svc.cluster.local:9080/*\" }, \"per_filter_config\": { \"mixer\": { \"forward_attributes\": { \"attributes\": { \"destination.service.uid\": { \"string_value\": \"istio://default/services/details\" }, \"destination.service.host\": { \"string_value\": \"details.default.svc.cluster.local\" }, \"destination.service.namespace\": { \"string_value\": \"default\" }, \"destination.service.name\": { \"string_value\": \"details\" }, \"destination.service\": { \"string_value\": \"details.default.svc.cluster.local\" } } }, \"mixer_attributes\": { \"attributes\": { \"destination.service.host\": { \"string_value\": \"details.default.svc.cluster.local\" }, \"destination.service.uid\": { \"string_value\": \"istio://default/services/details\" }, \"destination.service.name\": { \"string_value\": \"details\" }, \"destination.service.namespace\": { \"string_value\": \"default\" }, \"destination.service\": { \"string_value\": \"details.default.svc.cluster.local\" } } }, \"disable_check_calls\": true } } } ] } 关于 route.VirtualHost 的详细介绍请参考 Envoy v2 API reference - route.VirtualHost。 route.Route 路由既是如何匹配请求的规范，也是对下一步做什么的指示（例如，redirect、forward、rewrite等）。 route.Route 的数据结构 下面是是 route.Route 的数据结构，除了 match 之外其余都是可选的。 { \"match\": \"{...}\", \"route\": \"{...}\", \"redirect\": \"{...}\", \"direct_response\": \"{...}\", \"metadata\": \"{...}\", \"decorator\": \"{...}\", \"per_filter_config\": \"{...}\", \"request_headers_to_add\": [], \"request_headers_to_remove\": [], \"response_headers_to_add\": [], \"response_headers_to_remove\": [] } 下面是关于上述数据结构中的常用配置解析。 match：路由匹配参数。例如 URL prefix（前缀）、path（URL 的完整路径）、regex（规则表达式）等。 route：这里面配置路由的行为，可以是 route、redirect 和 direct_response，不过这里面没有专门的一个配置项用来配置以上三种行为，而是根据实际填充的配置项来确定的。例如在此处添加 cluster 配置则暗示路由动作为”route“，表示将流量路由到该 cluster。详情请参考 route.RouteAction。 decorator：被匹配的路由的修饰符，表示被匹配的虚拟主机和 URL。该配置里有且只有一个必须配置的项 operation，例如 details.default.svc.cluster.local:9080/*。 per_filter_config：这是一个 map 类型，per_filter_config 字段可用于为 filter 提供特定路由的配置。Map 的 key 应与 filleter 名称匹配，例如用于 HTTP buffer filter 的 envoy.buffer。该字段是特定于 filter 的，详情请参考 HTTP filter。 关于 route.Route 的详细介绍请参考 Envoy v2 API reference - route.Route。 参考 Envoy v2 API 概览 - servicemesher.com 监听器发现服务（LDS）- servicemesher.com 路由发现服务（RDS）- servicemesher.com 集群发现服务（CDS）- servicemesher.com Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/envoy-api.html":{"url":"data-plane/envoy-api.html","title":"Envoy API","keywords":"","body":"Envoy API Envoy 提供了如下的 API： CDS（Cluster Discovery Service）：集群发现服务 EDS（Endpoint Discovery Service）：端点发现服务 HDS（Health Discovery Service）：健康发现服务 LDS（Listener Discovery Service）：监听器发现服务 MS（Metric Service）：将 metric 推送到远端服务器 RLS（Rate Limit Service）：速率限制服务 RDS（Route Discovery Service）：路由发现服务 SDS（Secret Discovery Service）：秘钥发现服务 所有名称以 DS 结尾的服务统称为 xDS。 本书中仅讨论 v2 版本的 API，因为 Envoy 仍在不断开发和完善中，随着版本迭代也有可能新增一些 API，本章的重点在于 xDS 协议，关于 Envoy 的 API 的更多信息请参考 Envoy v2 APIs for developers。 Envoy xDS 协议 Envoy xDS 为 Istio 控制平面与数据平面通信的基本协议，只要代理支持该协议表达形式就可以创建自己 Sidecar 来替换 Envoy。这一章中将带大家了解 Envoy xDS。 Envoy 是 Istio Service Mesh 中默认的 Sidecar，Istio 在 Enovy 的基础上按照 Envoy 的 xDS 协议扩展了其控制平面，在讲到 Envoy xDS 协议之前还需要我们先熟悉下 Envoy 的基本术语。下面列举了 Envoy 里的基本术语及其数据结构解析，关于 Envoy 的详细介绍请参考 Envoy 官方文档，至于 Envoy 在 Service Mesh（不仅限于 Istio） 中是如何作为转发代理工作的请参考网易云刘超的这篇深入解读 Service Mesh 背后的技术细节 以及理解 Istio Service Mesh 中 Envoy 代理 Sidecar 注入及流量劫持，本文引用其中的一些观点，详细内容不再赘述。 图片 - Envoy proxy 架构图 关于 xDS 的版本 有一点需要大家注意，就是 Envoy 的 API 有 v1 和 v2 两个版本，从 Envoy 1.5.0 起 v2 API 就已经生产就绪了，为了能够让用户顺利的向 v2 版本的额 API 过度，Envoy 启动的时候设置了一个 --v2-config-only 的标志，Enovy 不同版本对 v1/v2 API 的支持详情请参考 Envoy v1 配置废弃时间表。 Envoy 的作者 Matt Klein 在 Service Mesh 中的通用数据平面 API 设计这篇文章中说明了 Envoy API v1 的历史及其缺点，还有 v2 的引入。v2 API 是 v1 的演进，而不是革命，它是 v1 功能的超集。 在 Istio 1.0 及以上版本中使用的是 Envoy 1.8.0-dev 版本，其支持 v2 的 API，同时在 Envoy 作为 Sidecar proxy 启动的使用使用了例如下面的命令： $ /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster ratings --service-node sidecar~172.33.14.2~ratings-v1-8558d4458d-ld8x9.default~default.svc.cluster.local --max-obj-name-len 189 --allow-unknown-fields -l warn --v2-config-only 上面是都 Bookinfo 示例中的 rating pod 中的 sidecar 启动的分析，可以看到其中指定了 --v2-config-only，表明 Istio 1.0+ 只支持 xDS v2 的 API。 REST-JSON & gPRC API 单个的基本 xDS 订阅服务，如 CDS、EDS、LDS、RDS、SDS 同时支持 REST-JSON 和 gRPC API 配置。高级 API，如 HDS、ADS 和 EDS 多维 LB 仅支持 gRPC。这是为了避免将复杂的双向流语义映射到 REST。详见 Envoy v2 APIs for developers。 参考 Service Mesh 中的通用数据平面 API 设计 - servicemesher.com Envoy v2 APIs for developers - github.com Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/envoy-xds-protocol.html":{"url":"data-plane/envoy-xds-protocol.html","title":"xDS 协议解析","keywords":"","body":"xDS 协议解析 本文译自 xDS REST and gRPC protocol，译者：狄卫华，审校：宋净超 Envoy 通过查询文件或管理服务器来动态发现资源。概括地讲，对应的发现服务及其相应的 API 被称作 xDS。Envoy 通过订阅（subscription）方式来获取资源，如监控指定路径下的文件、启动 gRPC 流或轮询 REST-JSON URL。后两种方式会发送 DiscoveryRequest 请求消息，发现的对应资源则包含在响应消息 DiscoveryResponse 中。下面，我们将具体讨论每种订阅类型。 文件订阅 发现动态资源的最简单方式就是将其保存于文件，并将路径配置在 ConfigSource 中的 path 参数中。Envoy 使用 inotify（Mac OS X 上为 kqueue）来监控文件的变化，在文件被更新时，Envoy 读取保存的 DiscoveryResponse 数据进行解析，数据格式可以为二进制 protobuf、JSON、YAML 和协议文本等。 译者注：core.ConfigSource 配置格式如下： { \"path\": \"...\", \"api_config_source\": \"{...}\", \"ads\": \"{...}\" } 文件订阅方式可提供统计数据和日志信息，但是缺少 ACK/NACK 更新的机制。如果更新的配置被拒绝，xDS API 则继续使用最后一个的有效配置。 gRPC 流式订阅 单资源类型发现 每个 xDS API 可以单独配置 ApiConfigSource，指向对应的上游管理服务器的集群地址。每个 xDS 资源类型会启动一个独立的双向 gRPC 流，可能对应不同的管理服务器。API 交付方式采用最终一致性。可以参考后续聚合服务发现（ADS） 章节来了解必要的显式控制序列。 译者注：core.ApiConfigSource 配置格式如下： { \"api_type\": \"...\", \"cluster_names\": [], \"grpc_services\": [], \"refresh_delay\": \"{...}\", \"request_timeout\": \"{...}\" } 类型 URL 每个 xDS API 都与给定的资源的类型存在 1:1 对应。关系如下： LDS： envoy.api.v2.Listener RDS： envoy.api.v2.RouteConfiguration CDS： envoy.api.v2.Cluster EDS： envoy.api.v2.ClusterLoadAssignment SDS：envoy.api.v2.Auth.Secret 类型 URL 的概念如下所示，其采用 type.googleapis.com/ 的形式，例如 CDS 对应于 type.googleapis.com/envoy.api.v2.Cluster。在 Envoy 的请求和管理服务器的响应中，都包括了资源类型 URL。 ACK/NACK 和版本 每个 Envoy 流以 DiscoveryRequest 开始，包括了列表订阅的资源、订阅资源对应的类型 URL、节点标识符和空的 version_info。EDS 请求示例如下： version_info: node: { id: envoy } resource_names: - foo - bar type_url: type.googleapis.com/envoy.api.v2.ClusterLoadAssignment response_nonce: 管理服务器可立刻或等待资源就绪时发送 DiscoveryResponse作为响应，示例如下： version_info: X resources: - foo ClusterLoadAssignment proto encoding - bar ClusterLoadAssignment proto encoding type_url: type.googleapis.com/envoy.api.v2.ClusterLoadAssignment nonce: A Envoy 在处理 DiscoveryResponse 响应后，将通过流发送一个新的请求，请求包含应用成功的最后一个版本号和管理服务器提供的 nonce。如果本次更新已成功应用，则 version_info 的值设置为 X，如下序列图所示： 图片 - ACK 后的版本更新 在此序列图及后续中，将统一使用以下缩写格式： DiscoveryRequest： (V=version_info，R=resource_names，N=response_nonce，T=type_url) DiscoveryResponse： (V=version_info，R=resources，N=nonce，T=type_url) 译者注：在信息安全中，Nonce是一个在加密通信只能使用一次的数字。在认证协议中，它往往是一个随机或伪随机数，以避免重放攻击。Nonce也用于流密码以确保安全。如果需要使用相同的密钥加密一个以上的消息，就需要Nonce来确保不同的消息与该密钥加密的密钥流不同。（引用自维基百科）在本文中nonce是每次更新的数据包的唯一标识。 版本为 Envoy 和管理服务器提供了共享当前应用配置的概念和通过 ACK/NACK 来进行配置更新的机制。如果 Envoy 拒绝配置更新 X，则回复 error_detail 及前一个的版本号，在当前情况下为空的初始版本号，error_detail 包含了有关错误的更加详细的信息： 图片 - NACK 无版本更新 后续，API 更新可能会在新版本 Y 上成功： 图片 - ACK 紧接着 NACK 每个流都有自己的版本概念，但不存在跨资源类型的共享版本。在不使用 ADS 的情况下，每个资源类型可能具有不同的版本，因为 Envoy API 允许指向不同的 EDS/RDS 资源配置并对应不同的 ConfigSources。 何时发送更新 管理服务器应该只向 Envoy 客户端发送上次 DiscoveryResponse 后更新过的资源。Envoy 则会根据接受或拒绝 DiscoveryResponse 的情况，立即回复包含 ACK/NACK 的 DiscoveryRequest 请求。如果管理服务器每次发送相同的资源集结果，而不是根据其更新情况，则会导致 Envoy 和管理服务器通讯效率大打折扣。 在同一个流中，新的 DiscoveryRequests 将取代此前具有相同的资源类型 DiscoveryRequest 请求。这意味着管理服务器只需要响应给定资源类型最新的 DiscoveryRequest 请求即可。 资源提示 DiscoveryRequest 中的 resource_names 信息作为资源提示出现。一些资源类型，例如 Cluster 和 Listener 将使用一个空的 resource_names，因为 Envoy 需要获取管理服务器对应于节点标识的所有 Cluster（CDS）和 Listener（LDS）。对于其他资源类型，如 RouteConfigurations（RDS）和 ClusterLoadAssignments（EDS），则遵循此前的 CDS/LDS 更新，Envoy 能够明确地枚举这些资源。 LDS/CDS 资源提示信息将始终为空，并且期望管理服务器的每个响应都提供 LDS/CDS 资源的完整状态。缺席的 Listener 或 Cluster 将被删除。 对于 EDS/RDS，管理服务器并不需要为每个请求的资源进行响应，而且还可能提供额外未请求的资源。resource_names 只是一个提示。Envoy 将默默地忽略返回的多余资源。如果请求的资源中缺少相应的 RDS 或 EDS 更新，Envoy 将保留对应资源的最后的值。管理服务器可能会依据 DiscoveryRequest 中 node 标识推断其所需的 EDS/RDS 资源，在这种情况下，提示信息可能会被丢弃。从相应的角度来看，空的 EDS/RDS DiscoveryResponse 响应实际上是表明在 Envoy 中为一个空的资源。 当 Listener 或 Cluster 被删除时，其对应的 EDS 和 RDS 资源也需要在 Envoy 实例中删除。为使 EDS 资源被 Envoy 已知或跟踪，就必须存在应用过的 Cluster 定义（如通过 CDS 获取）。RDS 和 Listeners 之间存在类似的关系（如通过 LDS 获取）。 对于 EDS/RDS ，Envoy 可以为每个给定类型的资源生成不同的流（如每个 ConfigSource 都有自己的上游管理服务器的集群）或当指定资源类型的请求发送到同一个管理服务器的时候，允许将多个资源请求组合在一起发送。虽然可以单个实现，但管理服务器应具备处理每个给定资源类型中对单个或多个 resource_names 请求的能力。下面的两个序列图对于获取两个 EDS 资源都是有效的 {foo，bar}： 资源更新 如上所述，Envoy 可能会更新 DiscoveryRequest 中出现的 resource_names 列表，其中 DiscoveryRequest 是用来 ACK/NACK 管理服务器的特定的 DiscoveryResponse 。此外，Envoy 后续可能会发送额外的 DiscoveryRequests ，用于在特定 version_info 上使用新的资源提示来更新管理服务器。例如，如果 Envoy 在 EDS 版本 X 时仅知道集群 foo，但在随后收到的 CDS 更新时额外获取了集群 bar ，它可能会为版本 X 发出额外的 DiscoveryRequest 请求，并将 {foo，bar} 作为请求的 resource_names 。 图片 - CDS 响应导致 EDS 资源更新 这里可能会出现竞争状况；如果 Envoy 在版本 X 上发布了资源提示更新请求，但在管理服务器处理该请求之前发送了新的版本号为 Y 的响应，针对 version_info 为 X 的版本，资源提示更新可能会被解释为拒绝 Y 。为避免这种情况，通过使用管理服务器提供的 nonce，Envoy 可用来保证每个 DiscoveryRequest 对应到相应的 DiscoveryResponse ： 图片 - EDS 更新速率激发 nonces 管理服务器不应该为含有过期 nonce 的 DiscoveryRequest 发送 DiscoveryResponse 响应。在向 Envoy 发送的 DiscoveryResponse 中包含了的新 nonce ，则此前的 nonce 将过期。在资源新版本就绪之前，管理服务器不需要向 Envoy 发送更新。同版本的早期请求将会过期。在新版本就绪时，管理服务器可能会处理同一个版本号的多个 DiscoveryRequests请求。 图片 - 请求变的陈旧 上述资源更新序列表明 Envoy 并不能期待其发出的每个 DiscoveryRequest 都得到 DiscoveryResponse 响应。 最终一致性考虑 由于 Envoy 的 xDS API 采用最终一致性，因此在更新期间可能导致流量被丢弃。例如，如果通过 CDS/EDS 仅获取到了集群 X，而且 RouteConfiguration 引用了集群 X；在 CDS/EDS 更新集群 Y 配置之前，如果将 RouteConfiguration 将引用的集群调整为 Y ，那么流量将被吸入黑洞而丢弃，直至集群 Y 被 Envoy 实例获取。 对某些应用程序，可接受临时的流量丢弃，客户端重试或其他 Envoy sidecar 会掩盖流量丢弃。那些对流量丢弃不能容忍的场景，可以通过以下方式避免流量丢失，CDS/EDS 更新同时携带 X 和 Y ，然后发送 RDS 更新从 X 切换到 Y ，此后发送丢弃 X 的 CDS/EDS 更新。 一般来说，为避免流量丢弃，更新的顺序应该遵循 make before break 模型，其中 必须始终先推送 CDS 更新（如果有）。 EDS 更新（如果有）必须在相应集群的 CDS 更新后到达。 LDS 更新必须在相应的 CDS/EDS 更新后到达。 与新添加的监听器相关的 RDS 更新必须在最后到达。 最后，删除过期的 CDS 集群和相关的 EDS 端点（不再被引用的端点）。 如果没有新的集群/路由/监听器或者允许更新时临时流量丢失的情况下，可以独立推送 xDS 更新。请注意，在 LDS 更新的情况下，监听器须在接收流量之前被预热，例如如其配置了依赖的路由，则先需先从 RDS 进行获取。添加/删除/更新集群信息时，集群也需要进行预热。另一方面，如果管理平面确保路由更新时所引用的集群已经准备就绪，路由可以不用预热。 聚合服务发现（ADS） 当管理服务器进行资源分发时，通过上述保证交互顺序的方式来避免流量丢弃是一项很有挑战的工作。ADS 允许单一管理服务器通过单个 gRPC 流，提供所有的 API 更新。配合仔细规划的更新顺序，ADS 可规避更新过程中流量丢失。使用 ADS，在单个流上可通过类型 URL 来进行复用多个独立的 DiscoveryRequest/DiscoveryResponse 序列。对于任何给定类型的 URL，以上 DiscoveryRequest 和 DiscoveryResponse 消息序列都适用。 更新序列可能如下所示： 图片 - EDS/CDS 在一个 ADS 流上多路复用 每个 Envoy 实例可使用单独的 ADS 流。 最小化 ADS 配置的 bootstrap.yaml 片段示例如下： node: id: dynamic_resources: cds_config: {ads: {}} lds_config: {ads: {}} ads_config: api_type: GRPC grpc_services: envoy_grpc: cluster_name: ads_cluster static_resources: clusters: - name: ads_cluster connect_timeout: { seconds: 5 } type: STATIC hosts: - socket_address: address: port_value: lb_policy: ROUND_ROBIN http2_protocol_options: {} admin: ... 增量 xDS 增量 xDS 是可用于允许的 ADS、CDS 和 RDS 单独 xDS 端点： xDS 客户端对跟踪资源列表进行增量更新。这支持 Envoy 按需/惰性地请求额外资源。例如，当与未知集群相对应的请求到达时，可能会发生这种情况。 xDS 服务器可以增量更新客户端上的资源。这支持 xDS 资源可伸缩性的目标。管理服务器只需交付更改的单个集群，而不是在修改单个集群时交付所有上万个集群。 xDS 增量会话始终位于 gRPC 双向流的上下文中。这允许 xDS 服务器能够跟踪到连接的 xDS 客户端的状态。xDS REST 版本不支持增量。 在增量 xDS 中，nonce 字段是必需的，用于匹配 IncrementalDiscoveryResponse 关联的 ACK 或 NACK IncrementalDiscoveryRequest。可选地，存在响应消息级别的 system_version_info，但仅用于调试目的。 IncrementalDiscoveryRequest 可在以下 3 种情况下发送： xDS 双向 gRPC 流的初始消息。 作为对先前的 IncrementalDiscoveryResponse 的 ACK 或 NACK 响应。在这种情况下，response_nonce 被设置为响应中的 nonce 值。ACK 或 NACK 由可由 error_detail 字段是否出现来区分。 客户端自发的 IncrementalDiscoveryRequest。此场景下可以采用动态添加或删除被跟踪的 resource_names 集。这种场景下，必须忽略 response_nonce。 在第一个示例中，客户端连接并接收它的第一个更新并 ACK。第二次更新失败，客户端发送 NACK 拒绝更新。xDS客户端后续会自发地请求 “wc” 相关资源。 图片 - 增量 session 示例 在重新连接时，支持增量的 xDS 客户端可能会告诉服务器其已知资源从而避免通过网络重新发送它们。 图片 - 增量重连示例 REST-JSON 轮询订阅 单个 xDS API 可对 REST 端点进行同步（长）轮询。除了无持久流与管理服务器交互外，消息顺序与上述相似。在任何时间点，只存在一个未完成的请求，因此响应消息中的 nonce 在 REST-JSON 中是可选的。DiscoveryRequest 和 DiscoveryResponse 的消息编码遵循 JSON 变换 proto3 规范。ADS 不支持 REST-JSON 轮询。 当轮询期间设置为较小的值时，则可以等同于长轮询，这时要求避免发送 DiscoveryResponse，除非对请求的资源发生了更改。 Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/envoy-lds.html":{"url":"data-plane/envoy-lds.html","title":"LDS（监听器发现服务）","keywords":"","body":"LDS（监听器发现服务） Listener 发现服务（LDS）是一个可选的 API，Envoy 将调用它来动态获取 Listener。Envoy 将协调 API 响应，并根据需要添加、修改或删除已知的 Listener。 Listener 更新的语义如下： 每个 Listener 必须有一个独特的名字。如果没有提供名称，Envoy 将创建一个 UUID。要动态更新的 Listener ，管理服务必须提供 Listener 的唯一名称。 当一个 Listener 被添加，在参与连接处理之前，会先进入“预热”阶段。例如，如果 Listener 引用 RDS 配置，那么在 Listener 迁移到 “active” 之前，将会解析并提取该配置。 Listener 一旦创建，实际上就会保持不变。因此，更新 Listener 时，会创建一个全新的 Listener （使用相同的侦听套接字）。新增加的监听者都会通过上面所描述的相同“预热”过程。 当更新或删除 Listener 时，旧的 Listener 将被置于 “draining（逐出）” 状态，就像整个服务重新启动时一样。Listener 移除之后，该 Listener 所拥有的连接，经过一段时间优雅地关闭（如果可能的话）剩余的连接。逐出时间通过 --drain-time-s 选项设置。 注意 Envoy 从 1.9 版本开始已不再支持 v1 API。 v2 LDS API 统计 LDS 的统计树是以 listener_manager.lds 为根，统计如下： 名字 类型 描述 config_reload Counter 因配置不同而导致配置重新加载的总次数 update_attempt Counter 尝试调用配置加载 API 的总次数 update_success Counter 调用配置加载 API 成功的总次数 update_failure Counter 调用配置加载 API 因网络错误的失败总数 update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数 version Gauge 来自上次成功调用配置加载API的内容哈希 control_plane.connected_state Gauge 布尔值，用来表示与管理服务器的连接状态，1表示已连接，0表示断开连接 参考 Listener discovery service(LDS) - envoyproxy.io 监听器发现服务（LDS）- servicemesher.com Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/envoy-rds.html":{"url":"data-plane/envoy-rds.html","title":"RDS（路由发现服务）","keywords":"","body":"RDS（路由发现服务） 路由发现服务（RDS）是 Envoy 里面的一个可选 API，用于动态获取路由配置。路由配置包括 HTTP header 修改、虚拟主机以及每个虚拟主机中包含的单个路由规则配置。每个 HTTP 连接管理器都可以通过 API 独立地获取自身的路由配置。 v2 API 参考 注意：Envoy 从 1.9 版本开始已不再支持 v1 API。 统计 RDS 的统计树以 http..rds..*.为根，route_config_name名称中的任何:字符在统计树中被替换为_。统计树包含以下统计信息： 名字 类型 描述 config_reload Counter 因配置不同而导致配置重新加载的总次数 update_attempt Counter 尝试调用配置加载 API 的总次数 update_success Counter 调用配置加载 API 成功的总次数 update_failure Counter 调用配置加载 API 因网络错误的失败总数 update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数 version Gauge 来自上次成功调用配置加载API的内容哈希 参考 Route discovery service(RDS) - envoyproxy.io Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/envoy-cds.html":{"url":"data-plane/envoy-cds.html","title":"CDS（集群发现服务）","keywords":"","body":"CDS（集群发现服务） 集群发现服务（CDS）是一个可选的 API，Envoy 将调用该 API 来动态获取 cluster manager 的成员。Envoy 还将根据 API 响应协调集群管理，根据需要完成添加、修改或删除已知的集群。 关于 Envoy 是如何通过 CDS 从 pilot-discovery 服务中获取的 cluster 配置，请参考 Service Mesh深度学习系列part3—istio源码分析之pilot-discovery模块分析（续）一文中的 CDS 服务部分。 注意 在 Envoy 配置中静态定义的 cluster 不能通过 CDS API 进行修改或删除。 Envoy 从 1.9 版本开始已不再支持 v1 API。 v2 CDS API 统计 CDS 的统计树以 cluster_manager.cds. 为根，统计如下： 名字 类型 描述 config_reload Counter 因配置不同而导致配置重新加载的总次数 update_attempt Counter 尝试调用配置加载 API 的总次数 update_success Counter 调用配置加载 API 成功的总次数 update_failure Counter 调用配置加载 API 因网络错误的失败总数 update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数 version Gauge 来自上次成功调用配置加载API的内容哈希 control_plane.connected_state Gauge 布尔值，用来表示与管理服务器的连接状态，1表示已连接，0表示断开连接 参考 Service Mesh深度学习系列part3—istio源码分析之pilot-discovery模块分析（续）- servicemesher.com Cluster discovery service - envoyproxy.io Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/envoy-eds.html":{"url":"data-plane/envoy-eds.html","title":"EDS（端点发现服务）","keywords":"","body":"EDS（端点发现服务） EDS 只是 Envoy 中众多的服务发现方式的一种。要想了解 EDS 首先我们需要先知道什么是 Endpoint。 Endpoint Endpoint 即上游主机标识。它的数据结构如下： { \"address\": \"{...}\", \"health_check_config\": \"{...}\" } 其中包括端点的地址和健康检查配置。详情请参考 endpoint.Endpoint。 终端发现服务（EDS）是一个基于 gRPC 或 REST-JSON API 服务器的 xDS 管理服务，在 Envoy 中用来获取集群成员。集群成员在 Envoy 的术语中被称为“终端”。对于每个集群，Envoy 都会通过发现服务来获取成员的终端。由于以下几个原因，EDS 是首选的服务发现机制： Envoy 对每个上游主机都有明确的了解（与通过 DNS 解析的负载均衡进行路由相比而言），并可以做出更智能的负载均衡决策。 在每个主机的发现 API 响应中携带的额外属性通知 Envoy 负载均衡权重、金丝雀状态、区域等。这些附加属性在负载均衡、统计信息收集等过程中会被 Envoy 网格全局使用。 Envoy 提供了 Java 和 Go 语言版本的 EDS 和其他发现服务的参考 gRPC 实现。 通常，主动健康检查与最终一致的服务发现服务数据结合使用，以进行负载均衡和路由决策。 参考 Service Mesh深度学习系列part3—istio源码分析之pilot-discovery模块分析（续）- servicemesher.com EDS - envoyproxy.io endpoint.Endpoint - envoyproxy.io Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/envoy-sds.html":{"url":"data-plane/envoy-sds.html","title":"SDS（秘钥发现服务）","keywords":"","body":"SDS（秘钥发现服务） SDS（秘钥发现服务）是 Envoy 1.8.0 版本起开始引入的服务。可以在 bootstrap.static_resource 的 secrets 配置中为 Envoy 指定 TLS 证书（secret）。也可以通过秘钥发现服务（SDS）远程获取。Istio 预计将在 1.1 版本中支持 SDS。 SDS 带来的最大的好处就是简化证书管理。要是没有该功能的话，我们就必须使用 Kubernetes 中的 secret 资源创建证书，然后把证书挂载到代理容器中。如果证书过期，还需要更新 secret 和需要重新部署代理容器。使用 SDS，中央 SDS 服务器将证书推送到所有 Envoy 实例上。如果证书过期，服务器只需将新证书推送到 Envoy 实例，Envoy 可以立即使用新证书而无需重新部署。 如果 listener server 需要从远程获取证书，则 listener server 不会被标记为 active 状态，在获取证书之前不会打开其端口。如果 Envoy 由于连接失败或错误的响应数据而无法获取证书，则 listener server 将被标记为 active，并且打开端口，但是将重置与端口的连接。 上游集群的处理方式类似，如果需要通过 SDS 从远程获取集群客户端证书，则不会将其标记为 active 状态，在获得证书之前也它不会被使用。如果 Envoy 由于连接失败或错误的响应数据而无法获取证书，则集群将被标记为 active，可以处理请求，但路由到该集群的请求都将被拒绝。 使用 SDS 的静态集群需定义 SDS 集群（除非使用不需要集群的 Google gRPC），则必须在使用静态集群之前定义 SDS 集群。 Envoy 代理和 SDS 服务器之间的连接必须是安全的。可以在同一主机上运行 SDS 服务器，使用 Unix Domain Socket 进行连接。否则，需要代理和 SDS 服务器之间的 mTLS。在这种情况下，必须静态配置 SDS 连接的客户端证书。 SDS Server SDS server 需要实现 SecretDiscoveryService 这个 gRPC 服务。遵循与其他 xDS 相同的协议。 SDS 配置 SDS 支持静态配置也支持动态配置。 静态配置 可以在static_resources 的 secrets 中配置 TLS 证书。 动态配置 从远程 SDS server 获取 secret。 通过 Unix Domain Socket 访问 gRPC SDS server。 通过 UDS 访问 gRPC SDS server。 通过 Envoy gRPC 访问 SDS server。 配置详情请参考 Envoy 官方文档。 参考 Secret discovery service (SDS) - envoyproxy.io Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/envoy-ads.html":{"url":"data-plane/envoy-ads.html","title":"ADS（聚合发现服务）","keywords":"","body":"ADS（聚合发现服务） 虽然 Envoy 本质上采用了最终一致性模型，但 ADS 提供了对 API 更新推送进行排序的机会，并确保单个管理服务器对 Envoy 节点的 API 更新具有亲和力。ADS 允许管理服务器在单个双向 gRPC 流上传递一个或多个 API 及其资源。否则，一些 API（如 RDS 和 EDS）可能需要管理多个流并连接到不同的管理服务器。 ADS 通过适当得排序 xDS 可以无中断的更新 Enovy 的配置。例如，假设 foo.com 已映射到集群 X。我们希望将路由表中将该映射更改为在集群 Y。为此，必须首先提供 X、Y 这两个集群的 CDS/EDS 更新。 如果没有 ADS，CDS/EDS/RDS 流可能指向不同的管理服务器，或者位于需要协调的不同 gRPC流连接的同一管理服务器上。EDS 资源请求可以跨两个不同的流分开，一个用于 X，一个用于 Y。ADS 将这些流合并到单个流和单个管理服务器，从而无需分布式同步就可以正确地对更新进行排序。使用 ADS，管理服务器将在单个流上提供 CDS、EDS 和 RDS 更新。 ADS 仅适用于 gRPC 流（非REST），本文档对此进行了更全面的描述。 参考 Aggregated Discovery Service - envoyproxy.io Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/envoy-hds.html":{"url":"data-plane/envoy-hds.html","title":"HDS（健康发现服务）","keywords":"","body":"HDS（健康发现服务） HDS（健康发现服务）支持管理服务器对其管理的 Envoy 实例进行高效的端点健康发现。单个 Envoy 实例通常会收到 HDS 指令，以检查所有端点的子集（subset）。运行状况检查子集可能并不是 Envoy 实例 EDS 端点的子集。 参考 Health Discovery Service (HDS) - github.com Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/envoy-advance-api.html":{"url":"data-plane/envoy-advance-api.html","title":"Envoy 高级 API","keywords":"","body":"Envoy 高级 API 除了 xDS API，Envoy 还提供如下高级 API： MS（Metric Service）：将 metric 推送到远端服务器 RLS（Rate Limit Service）：速率限制服务 Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/envoy-ms.html":{"url":"data-plane/envoy-ms.html","title":"MS（Metric 服务）","keywords":"","body":"MS（Metric 服务） Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"data-plane/envoy-rls.html":{"url":"data-plane/envoy-rls.html","title":"RLS（速率限制服务）","keywords":"","body":"RLS（速率限制服务） Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"setup/quick-start.html":{"url":"setup/quick-start.html","title":"快速开始","keywords":"","body":"快速开始 在使用 Istio 前还是希望您有容器和 Kubernetes 的基础知识，如果您想要从零开始，那么可以使用 kubernetes-vagrant-centos-cluster 并运行 Bookinfo 应用来快速体验服务网格。 当我们需要在本地开发时，更希望能够有一个开箱即用又可以方便定制的分布式开发环境，这样才能对Kubernetes本身和应用进行更好的测试。现在我们使用Vagrant和VirtualBox来创建一个这样的环境。 注意：kube-proxy使用ipvs模式。 Demo 点击下面的图片观看视频。 图片 - 观看视频 准备环境 需要准备以下软件和环境： 8G以上内存 Vagrant 2.0+ VirtualBox 5.0 + 提前下载Kubernetes 1.9以上版本（支持最新的1.13.0）的release压缩包 Mac/Linux，Windows不完全支持，仅在windows10下通过 集群 我们使用 Vagrant 和 Virtualbox 安装包含 3 个节点的 Kubernetes 集群，其中 master 节点同时作为 node 节点。 IP 主机名 组件 172.17.8.101 node1 kube-apiserver、kube-controller-manager、kube-scheduler、etcd、kubelet、docker、flannel、dashboard 172.17.8.102 node2 kubelet、docker、flannel、traefik 172.17.8.103 node3 kubelet、docker、flannel 注意：以上的IP、主机名和组件都是固定在这些节点的，即使销毁后下次使用vagrant重建依然保持不变。 容器 IP 地址范围：172.33.0.0/30 Kubernetes service IP 地址范围：10.254.0.0/16 安装的组件 安装完成后的集群包含以下组件： flannel（host-gw模式） kubernetes dashboard etcd（单节点） kubectl CoreDNS kubernetes（版本根据下载的kubernetes安装包而定，支持Kubernetes1.9+） 可选插件 Heapster + InfluxDB + Grafana ElasticSearch + Fluentd + Kibana Istio service mesh Helm Vistio Kiali 使用说明 将该repo克隆到本地，下载Kubernetes的到项目的根目录。 git clone https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster.git cd kubernetes-vagrant-centos-cluster 注意：如果您是第一次运行该部署程序，那么可以直接执行下面的命令，它将自动帮你下载 Kubernetes 安装包，下一次你就不需要自己下载了，另外您也可以在这里找到Kubernetes的发行版下载地址，下载 Kubernetes发行版后重命名为kubernetes-server-linux-amd64.tar.gz，并移动到该项目的根目录下。 因为该项目是使用 NFS 的方式挂载到虚拟机的 /vagrant 目录中的，所以在安装 NFS 的时候需要您输入密码授权。 使用vagrant启动集群。 vagrant up 如果是首次部署，会自动下载centos/7的box，这需要花费一些时间，另外每个节点还需要下载安装一系列软件包，整个过程大概需要10几分钟。 如果您在运行vagrant up的过程中发现无法下载centos/7的box，可以手动下载后将其添加到vagrant中。 手动添加centos/7 box wget -c http://cloud.centos.org/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1801_02.VirtualBox.box vagrant box add CentOS-7-x86_64-Vagrant-1801_02.VirtualBox.box --name centos/7 这样下次运行vagrant up的时候就会自动读取本地的centos/7 box而不会再到网上下载。 Windows 安装特别说明 执行vagrant up之后会有如下提示： G:\\code\\kubernetes-vagrant-centos-cluster>vagrant up Bringing machine 'node1' up with 'virtualbox' provider... Bringing machine 'node2' up with 'virtualbox' provider... Bringing machine 'node3' up with 'virtualbox' provider... ==> node1: Importing base box 'centos/7'... ==> node1: Matching MAC address for NAT networking... ==> node1: Setting the name of the VM: node1 ==> node1: Clearing any previously set network interfaces... ==> node1: Specific bridge 'en0: Wi-Fi (AirPort)' not found. You may be asked to specify ==> node1: which network to bridge to. ==> node1: Available bridged network interfaces: 1) Realtek PCIe GBE Family Controller 2) TAP-Windows Adapter V9 ==> node1: When choosing an interface, it is usually the one that is ==> node1: being used to connect to the internet. node1: Which interface should the network bridge to? node1: Which interface should the network bridge to? 输入1之后按回车继续。（根据自己真实网卡选择，node2、node3同样需要） node3快要结束的时候可能会有如下错误： node3: Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service. node3: Created symlink from /etc/systemd/system/multi-user.target.wants/kube-proxy.service to /usr/lib/systemd/system/kube-proxy.service. node3: deploy coredns node3: /tmp/vagrant-shell: ./dns-deploy.sh: /bin/bash^M: bad interpreter: No such file or directory node3: error: no objects passed to apply node3: /home/vagrant 解决方法： vagrant ssh node3 sudo -i cd /vagrant/addon/dns yum -y install dos2unix dos2unix dns-deploy.sh ./dns-deploy.sh -r 10.254.0.0/16 -i 10.254.0.2 |kubectl apply -f - 访问kubernetes集群 访问Kubernetes集群的方式有三种： 本地访问 在VM内部访问 Kubernetes dashboard 通过本地访问 可以直接在你自己的本地环境中操作该kubernetes集群，而无需登录到虚拟机中。 要想在本地直接操作Kubernetes集群，需要在你的电脑里安装kubectl命令行工具，对于Mac用户执行以下步骤： wget https://storage.googleapis.com/kubernetes-release/release/v1.11.0/kubernetes-client-darwin-amd64.tar.gz tar xvf kubernetes-client-darwin-amd64.tar.gz && cp kubernetes/client/bin/kubectl /usr/local/bin 将conf/admin.kubeconfig文件放到~/.kube/config目录下即可在本地使用kubectl命令操作集群。 mkdir -p ~/.kube cp conf/admin.kubeconfig ~/.kube/config 我们推荐您使用这种方式。 在虚拟机内部访问 如果有任何问题可以登录到虚拟机内部调试： vagrant ssh node1 sudo -i kubectl get nodes Kubernetes dashboard 还可以直接通过dashboard UI来访问：https://172.17.8.101:8443 可以在本地执行以下命令获取token的值（需要提前安装kubectl）： kubectl -n kube-system describe secret `kubectl -n kube-system get secret|grep admin-token|cut -d \" \" -f1`|grep \"token:\"|tr -s \" \"|cut -d \" \" -f2 注意：token的值也可以在vagrant up的日志的最后看到。 图片 - Kubernetes dashboard 只有当你安装了下面的heapster组件后才能看到上图中的监控metrics。 Windows下Chrome/Firefox访问 如果提示NET::ERR_CERT_INVALID，则需要下面的步骤 进入本项目目录 vagrant ssh node1 sudo -i cd /vagrant/addon/dashboard/ mkdir certs openssl req -nodes -newkey rsa:2048 -keyout certs/dashboard.key -out certs/dashboard.csr -subj \"/C=/ST=/L=/O=/OU=/CN=kubernetes-dashboard\" openssl x509 -req -sha256 -days 365 -in certs/dashboard.csr -signkey certs/dashboard.key -out certs/dashboard.crt kubectl delete secret kubernetes-dashboard-certs -n kube-system kubectl create secret generic kubernetes-dashboard-certs --from-file=certs -n kube-system kubectl delete pods $(kubectl get pods -n kube-system|grep kubernetes-dashboard|awk '{print $1}') -n kube-system #重新创建dashboard 刷新浏览器之后点击高级，选择跳过即可打开页面。 组件 Heapster监控 创建Heapster监控： kubectl apply -f addon/heapster/ 访问Grafana 使用Ingress方式暴露的服务，在本地/etc/hosts中增加一条配置： 172.17.8.102 grafana.jimmysong.io 访问Grafana：http://grafana.jimmysong.io 图片 - Grafana 界面 Traefik 部署Traefik ingress controller和增加ingress配置： kubectl apply -f addon/traefik-ingress 在本地/etc/hosts中增加一条配置： 172.17.8.102 traefik.jimmysong.io 访问Traefik UI：http://traefik.jimmysong.io 图片 - Traefik Ingress controller EFK 使用EFK做日志收集。 kubectl apply -f addon/efk/ 注意：运行EFK的每个节点需要消耗很大的CPU和内存，请保证每台虚拟机至少分配了4G内存。 Helm 用来部署helm。 hack/deploy-helm.sh Service Mesh 我们使用 istio 作为 service mesh。 安装 到Istio release 页面下载istio的安装包，安装istio命令行工具，将istioctl命令行工具放到你的$PATH目录下，对于Mac用户： wget https://github.com/istio/istio/releases/download/1.0.0/istio-1.0.0-osx.tar.gz tar xvf istio-1.0.0-osx.tar.gz mv bin/istioctl /usr/local/bin/ 在Kubernetes中部署istio： kubectl apply -f addon/istio/istio-demo.yaml kubectl apply -f addon/istio/istio-ingress.yaml 运行示例 我们开启了Sidecar自动注入。 kubectl label namespace default istio-injection=enabled kubectl apply -n default -f yaml/istio-bookinfo/bookinfo.yaml kubectl apply -n default -f yaml/istio-bookinfo/bookinfo-gateway.yaml kubectl apply -n default -f yaml/istio-bookinfo/destination-rule-all.yaml 在您自己的本地主机的/etc/hosts文件中增加如下配置项。 172.17.8.102 grafana.istio.jimmysong.io 172.17.8.102 prometheus.istio.jimmysong.io 172.17.8.102 servicegraph.istio.jimmysong.io 172.17.8.102 jaeger-query.istio.jimmysong.io 我们可以通过下面的URL地址访问以上的服务。 Service URL grafana http://grafana.istio.jimmysong.io servicegraph http://servicegraph.istio.jimmysong.io/dotviz, http://servicegraph.istio.jimmysong.io/graph,http://servicegraph.istio.jimmysong.io/force/forcegraph.html tracing http://jaeger-query.istio.jimmysong.io productpage http://172.17.8.101:31380/productpage 详细信息请参阅：https://istio.io/zh/docs/examples/bookinfo/ 图片 - Bookinfo Demo Vistio（可选安装） Vizceral是Netflix发布的一个开源项目，用于近乎实时地监控应用程序和集群之间的网络流量。Vistio是使用Vizceral对Istio和网格监控的改进。它利用Istio Mixer生成的指标，然后将其输入Prometheus。Vistio查询Prometheus并将数据存储在本地以允许重播流量。 # Deploy vistio via kubectl kubectl -n default apply -f addon/vistio/ # Expose vistio-api kubectl -n default port-forward $(kubectl -n default get pod -l app=vistio-api -o jsonpath='{.items[0].metadata.name}') 9091:9091 & # Expose vistio in another terminal window kubectl -n default port-forward $(kubectl -n default get pod -l app=vistio-web -o jsonpath='{.items[0].metadata.name}') 8080:8080 & 如果一切都已经启动并准备就绪，您就可以访问Vistio UI，开始探索服务网格网络，访问http://localhost:8080 您将会看到类似下图的输出。 图片 - vistio 页面 更多详细内容请参考Vistio—使用Netflix的Vizceral可视化Istio service mesh。 Kiali（可选安装） Kiali是一个用于提供Istio service mesh观察性的项目，更多信息请查看https://kiali.io。 在本地该项目的根路径下执行下面的命令： kubectl apply -n istio-system -f addon/kiali 图片 - kiali 注意：当前还不支持jeager追踪，请使用上文中提到的jeager地址。 Weave scope（可选安装） Weave scope可用于监控、可视化和管理Docker&Kubernetes集群，详情见https://www.weave.works/oss/scope/ 在本地该项目的根路径下执行下面的命令： kubectl apply -f addon/weave-scope 在本地的/etc/hosts下增加一条记录。 172.17.8.102 scope.weave.jimmysong.io 现在打开浏览器，访问http://scope.weave.jimmysong.io/ 图片 - Weave scope动画 管理 除了特别说明，以下命令都在当前的repo目录下操作。 挂起 将当前的虚拟机挂起，以便下次恢复。 vagrant suspend 恢复 恢复虚拟机的上次状态。 vagrant resume 注意：我们每次挂起虚拟机后再重新启动它们的时候，看到的虚拟机中的时间依然是挂载时候的时间，这样将导致监控查看起来比较麻烦。因此请考虑先停机再重新启动虚拟机。 重启 停机后重启启动。 vagrant halt vagrant up # login to node1 vagrant ssh node1 # run the prosivision scripts /vagrant/hack/k8s-init.sh exit # login to node2 vagrant ssh node2 # run the prosivision scripts /vagrant/hack/k8s-init.sh exit # login to node3 vagrant ssh node3 # run the prosivision scripts /vagrant/hack/k8s-init.sh sudo -i cd /vagrant/hack ./deploy-base-services.sh exit 现在你已经拥有一个完整的基础的kubernetes运行环境，在该repo的根目录下执行下面的命令可以获取kubernetes dahsboard的admin用户的token。 hack/get-dashboard-token.sh 根据提示登录即可。 清理 清理虚拟机。 vagrant destroy rm -rf .vagrant 注意 仅做开发测试使用，不要在生产环境使用该项目。 参考 Kubernetes Handbook——Kubernetes中文指南/云原生应用架构实践手册 rootsongjc/kubernetes-vagrant-centos-cluster - github.com duffqiu/centos-vagrant - github.com coredns/deployment - github.com Kubernetes 1.8 kube-proxy 开启 ipvs - mritd.me Vistio—使用Netflix的Vizceral可视化Istio service mesh - servicemesher.com Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"setup/istio-installation.html":{"url":"setup/istio-installation.html","title":"Istio 安装","keywords":"","body":"Istio 安装 Istio 官方推荐使用 Helm 来安装，Istio 中的很多组件都可以选择安装或开启，因此 Helm chart 也是组合式的，下载 Istio 安装包后解压可以看到 install/kubernetes/helm/istio 目录下的 Helm chart 配置文件，参考 使用 Helm 进行安装。 Istio 的 helm chart 下图是 Istio Helm Chart 的配置大全，通过给不同的组件使用不同的着色给大家一个直观的体验。 图片 - Istio Helm Chart Cheatsheet(原图来自沈旭光) 上图可以通过 Google doc 下载电子表格。 Istio 的安装文件中包括如下几个子 chart。 ingress ingressgateway egressgateway sidecarInjectorWebhook galley mixer pilot security(citadel) grafana prometheus servicegraph tracing(jaeger) kiali 所有的这些子 Chart 都可以通过 YAML 配置中的 enabled 标志选择性的开启，具体配置方法请参考安装包解压后的 install/kubernetes/helm/istio/README.md 文件。 使用 Helm 安装 Istio 因为使用单个 YAML 文件安装 Istio 的可读性很低（动辄上万行的 YAML 文件），从 Istio 1.0 以后版本起，官方推荐使用 Helm 来安装 Istio，不过在 Istio 的安装包中仍提供直接可用的 YAML 文件，用户也可以使用 helm template 生成独立的 YAML 文件。 下面我们说明如何使用在安装有 tiller 的 Kubernetes 集群中使用 Helm 安装 Istio。 安装 Helm 在本文发表时，Helm 最新的稳定版本为 v2.11.0，本文假设您已经有一个 Kubernetes 集群，Helm 的详细安装步骤请参考 Helm 官方文档，下面简述了 Helm 的安装步骤。 1. 下载安装包 您可以从 Helm release 页面下载对应操作系统的安装包，以 Mac 系统为例，选择下载 MacOS amd64 的安装包。 2. 安装 helm 下载后解压出 helm 可执行文件，将其移动到您的 $PATH 路径下。 3. 设置 RBAC 因为 tiller 将于 Kubernetes API Server 通信，需要相应的角色和权限，需要设置 RBAC。 kubectl apply -f install/kubernetes/helm/helm-service-account.yaml 4. 安装 tiller 执行 helm init 安装 tiller，默认将从 gcr.io 下载 tiller 的镜像，若需要指定特定的镜像仓库可以使用如下面的命令： helm init -i jimmysong/kubernetes-helm-tiller:v2.11.0 --service-account tiller 5. 检查安装是否完成 helm version Client: &version.Version{SemVer:\"v2.11.0\", GitCommit:\"2e55dbe1fdb5fdb96b75ff144a339489417b146b\", GitTreeState:\"clean\"} Server: &version.Version{SemVer:\"v2.11.0\", GitCommit:\"2e55dbe1fdb5fdb96b75ff144a339489417b146b\", GitTreeState:\"clean\"} 若没有看到报错信息则表示安装完成。 安装 Istio 在 Istio release 页面下载与您的操作系统对应的安装包，解压之后将 bin/istioctl 移动到您的 $PATH 目录下。 然后使用 helm 命令安装 Istio。 helm install install/kubernetes/helm/istio --name istio --namespace istio-system Istio 将被安装到 istio-system 的 namespace 下。 Istio 中的 CRD 安装完 Istio 后我们再查看下 Istio 创建的与网络相关的 Kubernetes CRD（自定义资源类型），请参考使用自定义资源扩展 API。Istio 创建的所有的 Kubernetes CRD 可以这样查看。 kubectl get customresourcedefinition|grep istio.io 你将会看到 50 个 CRD，其实要想了解 Istio 控制平面是怎样工作的，只需要了解这 50 个 CRD 是怎么工作的即可，很遗憾目前 Istio 还没有推出 API 文档。根据 API 的域名看到所有的 CRD 分为四类。 authentication：策略管控 config：配置分发与遥测 networking：流量管理 rbac：基于角色的访问控制 详细列表如下： # authentication，这两个 CRD 不是直接在 YAML 里定义的 meshpolicies.authentication.istio.io policies.authentication.istio.io # config adapters.config.istio.io apikeys.config.istio.io attributemanifests.config.istio.io authorizations.config.istio.io bypasses.config.istio.io checknothings.config.istio.io circonuses.config.istio.io deniers.config.istio.io edges.config.istio.io fluentds.config.istio.io handlers.config.istio.io httpapispecbindings.config.istio.io httpapispecs.config.istio.io instances.config.istio.io kubernetesenvs.config.istio.io kuberneteses.config.istio.io listcheckers.config.istio.io listentries.config.istio.io logentries.config.istio.io memquotas.config.istio.io metrics.config.istio.io noops.config.istio.io opas.config.istio.io prometheuses.config.istio.io quotas.config.istio.io quotaspecbindings.config.istio.io quotaspecs.config.istio.io rbacs.config.istio.io redisquotas.config.istio.io reportnothings.config.istio.io rules.config.istio.io servicecontrolreports.config.istio.io servicecontrols.config.istio.io signalfxs.config.istio.io solarwindses.config.istio.io stackdrivers.config.istio.io statsds.config.istio.io stdios.config.istio.io templates.config.istio.io tracespans.config.istio.io # networking destinationrules.networking.istio.io envoyfilters.networking.istio.io gateways.networking.istio.io serviceentries.networking.istio.io virtualservices.networking.istio.io # rbac rbacconfigs.rbac.istio.io servicerolebindings.rbac.istio.io serviceroles.rbac.istio.io 从中可以看出 config 类型的 CRD 是最多的，这是因为在 Mixer 中有众多的 adapter 导致，几十个 adapter 分别创建自己的适配器来对接基础设施后端。 CRD 的详细分类和用途如下图所示。 图片 - Istio CRD Cheatsheet(原图来自沈旭光) 上图可以通过 Google doc 下载电子表格。 参考 使用 Helm 进行安装 - istio.io Helm 官方文档 - helm.sh Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"setup/istio-observability-tool-kiali.html":{"url":"setup/istio-observability-tool-kiali.html","title":"可观察性工具 kiali","keywords":"","body":"可观察性工具 kiali Istio 中有个 issue #9066 要求将 Istio 中默认使用的 Service Graph 替换成 Kiali。Kiali 最初是由 Red Hat 开源的，用于解决 Service Mesh 中可观察性即微服务的可视性问题。目前已获得 Istio 社区的官方支持。 关于 Kiali 单体应用使用微服务架构拆分成了许多微服务的组合。服务的数量显著增加，就对需要了解服务之间的通信模式，例如容错（通过超时、重试、断路等）以及分布式跟踪，以便能够看到服务调用的去向。服务网格可以在平台级别上提供这些服务，并使应用程序编写者从以上繁重的通信模式中解放出来。路由决策在网格级别完成。Kiali 与Istio 合作，可视化服务网格拓扑、断路器和请求率等功能。Kiali还包括 Jaeger Tracing，可以提供开箱即用的分布式跟踪功能。 Kiali 提供的功能 Kiali 提供以下功能： 服务拓扑图 分布式跟踪 指标度量收集和图标 配置校验 健康检查和显示 服务发现 下图展示了 kiali 中显示的 Bookinfo 示例的服务拓扑图。 你可以使用 kubernetes-vagrant-centos-cluster 来快速启动一个运行 Kiali 的 Kubernetes 集群。 编译安装与试用 Kilia pod 中运行的进程是 /opt/kiali/kiali -config /kiali-configuration/config.yaml -v 4。 /kiali-configuration/config.yaml 是使用 ConfigMap 挂载进去的，用于配置 Kiali 的 Web 根路径和外部服务地址。 server: port: 20001 web_root: / external_services: jaeger: url: \"http://172.17.8.101:31888\" grafana: url: \"http://grafana.istio-system:3000\" Kiali 中的基本概念 在了解 Kiali 如何提供 Service Mesh 中微服务可观察性之前，我们需要先了解下 Kiali 如何划分监控类别的。 Application：使用运行的工作负载，必须使用 Istio 的将 Label 标记为 app 才算。注意，如果一个应用有多个版本，只要 app 标签的值相同就是属于同一个应用。 Deployment：即 Kubernetes 中的 Deployment。 Label：这个值对于 Istio 很重要，因为 Istio 要用它来标记 metrics。每个 Application 要求包括 app 和 version 两个 label。 Namespace：通常用于区分项目和用户。 Service：即 Kubernetes 中的 Service，不过要求必须有 app label。 Workload：Kubernetes 中的所有常用资源类型如 Deployment、StatefulSet、Job 等都可以检测到，不论这些负载是否加入到 Istio Service Mesh 中。 Application、Workload 与 Service 的关系如下图所示。 Kilia 的详细 API 使用说明请查看 Swagger API 文档，在 Kiali 的根目录下运行下面的命令可以查看 API 文档。 make swagger-serve Swagger UI 如下图。 架构 Kiali 部署完成后只启动了一个 Pod，前后端都集成在这一个 Pod 中。Kiali 也有一些依赖的组件，例如如果要在 Kiali 的页面中获取到监控 metric 需要使用在 istio-system 中部署 Prometheus。分布式卓总直接下图是 Kiali 的架构，来自 Kiali 官网。 Kiali 使用传统的前后端分离架构： 后端使用 Go 编写：https://github.com/kiali/kiali，为前端提供 API，所有消息使用 JSON 编码，使用 ConfigMap 和 Secret 来存储配置。直接与 Kubernetes 和 Istio 通信来获取数据。 前端使用 Typescript 编写：https://github.com/kiali/kiali-ui，无状态，除了一些证书保存在浏览器中。于查询后端 API，可以跳转访问 Jaeger 分布式追踪和 Grafana 监控页面。 Jaeger 和 Grafana 都是可选组件，使用的都是外部服务，不是由 Kiali 部署的，需要在 kiali-configmap.yaml 中配置 URL。注意该 URL 必须是从你本地浏览器中可以直接访问到的地址。 注意：如果服务之间没有任何请求就不会在 Prometheus 中保存数据也就无法显示服务拓扑图，所以大家在部署完 Bookinfo 服务之后向 productpage 服务发送一些请求用于生成服务拓扑图。 服务拓扑图 Kiali 中的服务拓扑图比起 Istio 原来默认部署的 ServiceGraph 的效果更炫也更加直观，具有更多选项。 例如使用 CURL 模拟请求。 $ curl -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6ImFkbWluIiwiZXhwIjoxNTM5NjczOTYyfQ.6gNz4W6yA9Bih4RkTbcSvqdaiRqsyj8c8o6ictM9iDs\" http://172.17.8.101:32439/api/namespaces/all/graph?duration=60s&graphType=versionedApp&injectServiceNodes=false&appenders=dead_node,sidecars_check,istio 会得到如下的返回的 JSON 返回值，为了节省篇幅其中省略了部分结果： { \"timestamp\": 1539296648, \"graphType\": \"versionedApp\", \"elements\": { \"nodes\": [ { \"data\": { \"id\": \"6519157be154675342fb76c41edc731c\", \"nodeType\": \"app\", \"namespace\": \"default\", \"app\": \"reviews\", \"isGroup\": \"version\" } }, ... { \"data\": { \"id\": \"6249668dd0a91adb9e62994d36563365\", \"nodeType\": \"app\", \"namespace\": \"istio-system\", \"workload\": \"istio-ingressgateway\", \"app\": \"istio-ingressgateway\", \"version\": \"unknown\", \"rateOut\": \"0.691\", \"isOutside\": true, \"isRoot\": true } } ], \"edges\": [ { \"data\": { \"id\": \"d51ca2a95d721427bbe27ed209766ec5\", \"source\": \"06e488a37fc9aa5b0e0805db4f16ae69\", \"target\": \"31150e7e5adf85b63f22fbd8255803d7\", \"rate\": \"0.236\", \"percentRate\": \"17.089\", \"responseTime\": \"0.152\" } }, ... { \"data\": { \"id\": \"1dda06d9904bcf727d1b6a113be58556\", \"source\": \"80f71758099020586131c3565075935d\", \"target\": \"4b64bda48e5a3c7e50ab1c63836c9469\", \"rate\": \"0.236\", \"responseTime\": \"0.022\" } } ] } } 该值中包含了每个 node 和 edege 的信息，Node 即图中的每个节点，其中包含了节点的配置信息，Edge 即节点间的关系还有流量情况。前端可以根据该信息绘制服务拓扑图，我们下面将查看下 kiali 的后端，看看它是如何生成以上格式的 JSON 信息的。 注：详细的 REST API 使用和字段说明请查看 swagger 生成的 API 文档。 代码解析 下面将带大家了解 Kiali 的后端代码基本结构。 路由配置 服务拓扑图的路由信息保存在 kiali/routing/routes.go 文件中。 { \"GraphNamespace\", \"GET\", \"/api/namespaces/{namespace}/graph\", handlers.GraphNamespace, true, }, { \"GraphAppVersion\", \"GET\", \"/api/namespaces/{namespace}/applications/{app}/versions/{version}/graph\", handlers.GraphNode, true, }, { \"GraphApp\", \"GET\", \"/api/namespaces/{namespace}/applications/{app}/graph\", handlers.GraphNode, true, }, { \"GraphService\", \"GET\", \"/api/namespaces/{namespace}/services/{service}/graph\", handlers.GraphNode, true, }, { \"GraphWorkload\", \"GET\", \"/api/namespaces/{namespace}/workloads/{workload}/graph\", handlers.GraphNode, true, } 直接查看 Swagger 生成的 API 文档也可以。 PQL 查询语句构建 kiali/handlers/graph.go 中处理 HTTP 请求，服务拓扑图中所有的指标信息都是从 Prometheus 中查询得到的。 Kiali 的服务状态拓扑是根据 namespace 来查询的，例如 default namespace 下的服务指标查询 PQL： round(sum(rate(istio_requests_total{reporter=\"source\",source_workload_namespace=\"default\",response_code=~\"[2345][0-9][0-9]\"} [600s])) by (source_workload_namespace,source_workload,source_app,source_version,destination_service_namespace,destination_service_name,destination_workload,destination_app,destination_version,response_code),0.001) 其中的参数都是通过页面选择传入的（构建的 PQL 中的选项在 kiali/graph/options/options.go 中定义）： reporter=\"source\"：metric 报告来源，源服务（source）是 envoy 代理的下游客户端。在服务网格里，一个源服务通常是一个工作负载，但是入口流量的源服务有可能包含其他客户端，例如浏览器，或者一个移动应用。 source_workload_namespace=\"default\"：选择命名空间。 response_code：返回码区间。 [600s]：查询的数据中的时间间隔。 关于 PQL 的详细使用方式请参考 QUERY EXAMPLES - prometheus.io。 这里面包含了所有 workload 的流量信息，做简单的操作就可以计算出 application/service 的流量状况。 HTTP 处理逻辑 HTTP 请求的处理逻辑入口位于 kiali/handlers/graph.go，路径为： func graphNamespaces(o options.Options, client *prometheus.Client) graph.TrafficMap { switch o.Vendor { case \"cytoscape\": default: checkError(errors.New(fmt.Sprintf(\"Vendor [%s] not supported\", o.Vendor))) } log.Debugf(\"Build [%s] graph for [%v] namespaces [%s]\", o.GraphType, len(o.Namespaces), o.Namespaces) trafficMap := graph.NewTrafficMap() for _, namespace := range o.Namespaces { log.Debugf(\"Build traffic map for namespace [%s]\", namespace) namespaceTrafficMap := buildNamespaceTrafficMap(namespace, o, client) for _, a := range o.Appenders { a.AppendGraph(namespaceTrafficMap, namespace) // Appender 用于添加 service graph } mergeTrafficMaps(trafficMap, namespaceTrafficMap) //将不同的 namespace 下的服务状态合并 } // appender 用于添加/删除/修改 node 信息。操作完成后可以做出如下判断： // - 将其标记外来者（即不在请求的 namespace 中的 node） // - 将其标记内部流量制造者（即位于 namespace 中只有向外的 edge） markOutsiders(trafficMap, o) markTrafficGenerators(trafficMap) if graph.GraphTypeService == o.GraphType { trafficMap = reduceToServiceGraph(trafficMap) } return trafficMap } Appender 是一个接口，在 service graph 中注入详细的信息，它的定义如下： // Appender 由任何代码提供实现，以附加具有补充信息的 service graph。如果出错，appender应该执行 panic 并将其作为错误响应处理。 type Appender interface { // AppendGraph 在提供的 traffic map 上执行 appender 工作。Map 最初可能是空的。允许 appender 添加或删除映射条目。 AppendGraph(trafficMap graph.TrafficMap, namespace string) } Appender 位于 kiali/graph/appender 目录下，目前一共有如下实现： DeadNodeAppender：用于将不想要 node 从 service graph 中删除。 IstioAppender：获取指定 namespace 下 Istio 的详细信息，当前版本获取指定 namespace 下的 VirtualService 和 DestinationRule 信息。 ResponseTimeAppender：获取响应时间。 SecurityPolicyAppender：在 service graph 中添加安全性策略信息。 SidecarsCheckAppender：检查 Sidecar 的配置信息，例如 Pod 中是否有 App label。 UnusedNodeAppender：未加入 Service Mesh 的 node。 我们再来看下在 kiali/graph/graph.go 中定义的 TrafficMap 结构。 // TrafficMap 是 App 与 Node 之间的映射，每个节点都可选择保存 Edge 数据。Metadata 是用于保存任何期望的 node 或 edge 信息的通用映射。每个 app 节点应具有唯一的 namespace + workload。请注意，在同一 namespace 中有两个具有相同 name + version 的节点是可行的但可能并不常见。 type TrafficMap map[string]*Node type Node struct { ID string // unique identifier for the node NodeType string // Node type Namespace string // Namespace Workload string // Workload (deployment) name App string // Workload app label value Version string // Workload version label value Service string // Service name Edges []*Edge // child nodes Metadata map[string]interface{} // app-specific data } type Edge struct { Source *Node Dest *Node Metadata map[string]interface{} // app-specific data } 以上只是对 Kiali 部分代码的解读，更详细的实现大家可以克隆 kiali 的代码自己研究。 参考 Kiali.io QUERY EXAMPLES - prometheus.io replace Service Graph with Kiali #9066 - github.com rootsongjc/kubernetes-vagrant-centos-cluster - github.com Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"develop/istio-dev-env.html":{"url":"develop/istio-dev-env.html","title":"Istio 开发环境配置","keywords":"","body":"Istio 开发环境配置 本文将概述如何配置 Istio 的开发环境及编译和生成二进制文件和 Kubernetes 的 YAML 文件，更高级的测试、格式规范、原型和参考文档编写等请参考 Istio Dev Guide。 依赖环境 Istio 开发环境依赖以下软件： Docker：测试和运行时 Go 1.11：程序开发 fpm 包构建工具：用来打包 Kubernetes 1.7.3+ 设置环境变量 在编译过程中需要依赖以下环境变量，请根据你自己的 export ISTIO=$GOPATH/src/istio.io # DockerHub 的用户名 USER=jimmysong export HUB=\"docker.io/$USER\" # Docker 镜像的 tag，这里为了方便指定成了固定值，也可以使用 install/updateVersion.sh 来生成 tag export TAG=$USER # GitHub 的用户名 export GITHUB_USER=rootsongjc # 指定 Kubernetes 集群的配置文件地址 export KUBECONFIG=${HOME}/.kube/config 全量编译 编译过程中需要下载很多依赖包，请确认你的机器可以科学上网。 执行下面的命令可以编译 Istio 所有组件的二进制文件。 make 以在 Mac 下编译为例，编译完成后所有的二进制文件将位于 $GOPATH/out/darwin_amd64/release。 执行下面的命令构建镜像。 make docker 执行下面的命令将镜像推送到 DockerHub。 make push 也可以编译单独组件的镜像，详见开发指南。 构建 YAML 文件 执行下面的命令可以生成 YAML 文件。 make generate_yaml 生成的 YAML 文件位于 repo 根目录的 install/kubernetes 目录下。 参考 Istio Dev Guide - github.com Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "},"action/bookinfo-sample.html":{"url":"action/bookinfo-sample.html","title":"Bookinfo 示例","keywords":"","body":"Bookinfo 示例 Bookinfo 示例是 Istio 官方为了演示 Istio 功能而开发的一个示例应用，该应用有如下特点： 使用微服务方式开发，共有四个微服务 多语言应用，使用了 Java、Python、Ruby 和 NodeJs 语言 为了演示流量管理的高级功能，有的服务同时推出了多个版本 Bookinfo 应用部署架构 以下为 Istio 官方提供的该应用的架构图。 图片 - Istio 的 Bookinfo 示例应用架构图 Bookinfo 应用分为四个单独的微服务，其中每个微服务的部署的结构中都注入了一个 Sidecar： productpage ：productpage 微服务会调用 details 和 reviews 两个微服务，用来生成页面。 details ：这个微服务包含了书籍的信息。 reviews ：这个微服务包含了书籍相关的评论。它还会调用 ratings 微服务。 ratings ：ratings 微服务中包含了由书籍评价组成的评级信息。 reviews 微服务有 3 个版本： v1 版本不会调用 ratings 服务。 v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。 v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息。 使用 kubernetes-vagrant-centos-cluster 部署的 Kubernetes 集群和 Istio 服务的话可以直接运行下面的命令部署 Bookinfo 示例： $ kubectl apply -n default -f 关于该示例的介绍和详细步骤请参考 Bookinfo 应用。 Bookinfo 示例及 Istio 服务整体架构 从 Bookinfo 应用部署架构中可以看到该应用的几个微服务之间的关系，但是并没有描绘应用与 Istio 控制平面、Kubernetes 平台的关系，下图中描绘的是应用和平台整体的架构。 图片 - Bookinfo 示例与 Istio 的整体架构图 从图中可以看出 Istio 整体架构的特点： 模块化：很多模块可以选择性的开启，如负责证书管理的 istio-citadel 默认就没有启用 可定制化：可观察性的组件可以定制化和替换 参考 Bookinfo 应用 - istio.io Copyright © 2021 | Distributed under CC BY 4.0 | jimmysong.io all right reserved，powered by Gitbook Updated at 2021-04-07 20:26:18 "}}